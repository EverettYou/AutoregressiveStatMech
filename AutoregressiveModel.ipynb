{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'autoreg.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Model\n",
    "\n",
    "Autoregressive model is a generative model that models the joint probability distributition by product of conditional distributions\n",
    "$$p(x_1,x_2,x_3,\\cdots)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\cdots = \\prod_{i}p(x_i|x_1,\\cdots,x_{i-1}).$$\n",
    "The parameters of the coditional distributions will be calculated by neural networks. However, due to the autoregressive causal dependence of the conditional probability on the input variables, the neural network should be masked to respect the same causal structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Linear Layer\n",
    "\n",
    "A key component is to realize an autoregressive linear layer, which maps $x=(x_1,x_2,\\cdots)$ to $y=(y_1,y_2,\\cdots)$ via\n",
    "$$y = W\\cdot x + b,$$\n",
    "respecting the causality that $y_i$ only depends on $x_1,\\cdots,x_{i-1}$. This can be achieved by requiring the weight matrix $W$ to take a *lower-trianglar* form\n",
    "$$W = \\begin{bmatrix}\n",
    "0 & 0 & 0 & \\cdots & 0\\\\\n",
    "W_{21} & 0 & 0 & \\cdots & 0\\\\\n",
    "W_{31} & W_{32} & 0 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "W_{n1} & W_{n2} & W_{n3} & \\cdots & 0\\\\\n",
    "\\end{bmatrix}$$\n",
    "For PyTorch realization, we can first greate a raw weight matrix, which is a full matrix. Then construct the actual weight matrix by truncating the full matrix its low-triangle part. This can be implemented by `torch.tril` (which allows gradient backpropagate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example\n",
    "Create a full weight matrix `w_full` and truncate it to the triangular weight matrix `w_tril`. The function `torch.tril` takes a argument `diagonal` to specify the truncation to which diagonal (inclusively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 0.]], grad_fn=<TrilBackward>)\n"
     ]
    }
   ],
   "source": [
    "w_full = torch.ones(3, 3, requires_grad = True)\n",
    "w_tril = torch.tril(w_full, diagonal = -1)\n",
    "print(w_full)\n",
    "print(w_tril)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the triangular weight matrix in the remaining computation task to evaluate the loss function. For example, the loss funcion is simply the 2-norm of the triangular weight matrix (just to get some scalar score for the matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(w_tril**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can gradient back propagate and check how the raw weight matrix will receive the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [2., 0., 0.],\n",
       "        [2., 2., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "w_full.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the gadient is automatically masked as well. The upper triangle does not receive any gradient signal. If we put `w_full` into an optimizer to minimize the loss, the lower triangle of the weight matrix will be trained to zero (as favored by the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "        [-4.1566e-12,  1.0000e+00,  1.0000e+00],\n",
       "        [-4.1566e-12, -4.1566e-12,  1.0000e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam([w_full], lr = 0.1)\n",
    "for epoch in range(500):\n",
    "    w_tril = torch.tril(w_full, diagonal = -1)\n",
    "    loss = torch.sum(w_tril**2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "w_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack to Torch Module\n",
    "\n",
    "We can pack the above functionality to a Torch Module, inherited from `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveLinear(nn.Linear):\n",
    "    \"\"\" Applies a lienar transformation to the incoming data, \n",
    "        with the weight matrix masked to the lower-triangle. \n",
    "        \n",
    "        Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        diagonal: the diagonal to trucate to\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True, diagonal=0):\n",
    "        super(AutoregressiveLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.diagonal = diagonal\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return super(AutoregressiveLinear, self).extra_repr() + ', diagonal={}'.format(self.diagonal)\n",
    "    \n",
    "    # overwrite forward pass\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, torch.tril(self.weight, self.diagonal), self.bias)\n",
    "    \n",
    "    def forward_at(self, input, i):\n",
    "        output = input.matmul(torch.tril(self.weight, self.diagonal).narrow(0, i, 1).t())\n",
    "        if self.bias is not None:\n",
    "            output += self.bias.narrow(0, i, 1)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this module, let us create some data. The target $y$ is related to the input $x$ by $y_i=\\sum_{j=1}^{i}x_j$, which can be modeled by an autoregressive linear transformation, with weight being a lower-triangular matrix with all 1 below diagonal 0, and no bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2537,  1.2952, -0.7514,  1.6626,  0.2934],\n",
       "         [-0.8714, -2.4656,  0.7002, -0.4921, -0.5424],\n",
       "         [ 0.3850, -0.4022, -1.7203, -0.8830,  0.0989],\n",
       "         [-2.1567,  1.3889, -1.0016,  0.4516,  0.4419],\n",
       "         [ 0.6539,  1.0505,  0.3454,  1.2798, -0.0401],\n",
       "         [-0.9391,  0.2523,  0.1364, -1.1801,  1.5261],\n",
       "         [-0.8578,  0.0364, -0.0223, -0.0495, -1.2166],\n",
       "         [ 0.4616,  2.1005, -0.1037, -0.9643,  0.7355],\n",
       "         [ 0.3906, -0.9893,  0.9771, -0.6446, -0.0489],\n",
       "         [ 1.2490,  1.2904, -1.4693, -1.5764,  0.8918]]),\n",
       " tensor([[ 1.2537,  2.5489,  1.7975,  3.4601,  3.7535],\n",
       "         [-0.8714, -3.3371, -2.6368, -3.1289, -3.6714],\n",
       "         [ 0.3850, -0.0172, -1.7375, -2.6205, -2.5216],\n",
       "         [-2.1567, -0.7679, -1.7695, -1.3179, -0.8760],\n",
       "         [ 0.6539,  1.7044,  2.0497,  3.3296,  3.2895],\n",
       "         [-0.9391, -0.6868, -0.5505, -1.7306, -0.2045],\n",
       "         [-0.8578, -0.8213, -0.8436, -0.8931, -2.1097],\n",
       "         [ 0.4616,  2.5622,  2.4585,  1.4942,  2.2296],\n",
       "         [ 0.3906, -0.5987,  0.3784, -0.2661, -0.3150],\n",
       "         [ 1.2490,  2.5394,  1.0701, -0.5063,  0.3856]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(10, 5)\n",
    "target = torch.cumsum(input, axis = 1)\n",
    "input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning with mean-square-error (MSE) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.1874\n",
      "loss : 0.0000\n",
      "loss : 0.0000\n",
      "loss : 0.0000\n",
      "loss : 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = AutoregressiveLinear(5, 5)\n",
    "loss_op = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.)\n",
    "train_loss = 0.\n",
    "for epoch in range(500):\n",
    "    loss = loss_op(model(input), target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('loss : {:.4f}'.format(train_loss / 100))\n",
    "        train_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training converges, we inspect the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.0000,  0.4060, -0.0309, -0.4228, -0.0073],\n",
       "         [ 1.0000,  1.0000, -0.3395, -0.3059,  0.4422],\n",
       "         [ 1.0000,  1.0000,  1.0000,  0.3253, -0.0558],\n",
       "         [ 1.0000,  1.0000,  1.0000,  1.0000,  0.1358],\n",
       "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.4105e-08,  2.4496e-10, -2.0742e-07, -1.4842e-07, -5.9154e-08],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrix indeed becomes a lower-triangular matrix of 1's and the bias indeed vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model\n",
    "\n",
    "We can use autoregressive linear layers to build the autoregressive model. As a generative model, the autoregressive model must provide two functionalities:\n",
    "- `log_prob(input)` evaluating the log probability of a batch of samples as `input`,\n",
    "- `sample(batch_size)` generating a batch of samples given the `batch_size`, according to the model probability distribtuion.\n",
    "\n",
    "We realize these functionalities in the neural network module `AutoregressiveModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveModel(nn.Module):\n",
    "    \"\"\" Represent a generative model that can generate samples and provide log probability evaluations.\n",
    "        \n",
    "        Args:\n",
    "        features: size of each sample\n",
    "        depth: depth of the neural network (in number of linear layers) (default=1)\n",
    "        nonlinearity: activation function to use (default='ReLU') \"\"\"\n",
    "    \n",
    "    def __init__(self, features, depth=1, nonlinearity='ReLU'):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "        self.features = features # number of features\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            if i == 0: # first autoregressive linear layer must have diagonal=-1\n",
    "                self.layers.append(AutoregressiveLinear(self.features, self.features, diagonal = -1))\n",
    "            else: # remaining autoregressive linear layers have diagonal=0 (by default)\n",
    "                self.layers.append(AutoregressiveLinear(self.features, self.features))\n",
    "            if i == depth-1: # the last layer must be Sigmoid\n",
    "                self.layers.append(nn.Sigmoid())\n",
    "            else: # other layers use the specified nonlinearity\n",
    "                self.layers.append(getattr(nn, nonlinearity)())\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return '(features): {}'.format(self.features) + super(AutoregressiveModel, self).extra_repr()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        prob = input # prob as a workspace, initialized to input\n",
    "        for layer in self.layers: # apply layers\n",
    "            prob = layer(prob)\n",
    "        return prob # prob holds predicted Beroulli probability parameters\n",
    "    \n",
    "    def log_prob(self, input):\n",
    "        prob = self(input) # forward pass to get Beroulli probability parameters\n",
    "        return torch.sum(dist.Bernoulli(prob).log_prob(input), axis=-1)\n",
    "    \n",
    "    def sample(self, batch_size=1):\n",
    "        with torch.no_grad(): # no gradient for sample generation\n",
    "            # create a record to host layerwise outputs\n",
    "            record = torch.zeros(len(self.layers)+1, batch_size, self.features)\n",
    "            # autoregressive batch sampling\n",
    "            for i in range(self.features):\n",
    "                for l, layer in enumerate(self.layers):\n",
    "                    if isinstance(layer, AutoregressiveLinear): # linear layer\n",
    "                        record[l+1, :, i] = layer.forward_at(record[l], i)\n",
    "                    else: # elementwise layer\n",
    "                        record[l+1, :, i] = layer(record[l, :, i])\n",
    "                record[0, :, i] = dist.Bernoulli(record[-1, :, i]).sample()\n",
    "        return record[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "The conditional probabilities are modeled as Bernoulli distributions, whose probabilities are given by the autoregressive feed forward neural network. To respect the causal structure, the first autoregressive layer must have `diagonal=-1` such that $y_i$ depends on $(x_1,\\cdots,x_{i-1})$. To esure that the output are probabilities (real numbers between 0 and 1), the last layer nonlineary activation must be Sigmoid. The internal nonlinear activation can be specified freely (default: ReLU). The architecture of a depth-3 autoregressive model will be like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoregressiveModel(\n",
       "  (features): 5\n",
       "  (layers): ModuleList(\n",
       "    (0): AutoregressiveLinear(in_features=5, out_features=5, bias=True, diagonal=-1)\n",
       "    (1): ReLU()\n",
       "    (2): AutoregressiveLinear(in_features=5, out_features=5, bias=True, diagonal=0)\n",
       "    (3): ReLU()\n",
       "    (4): AutoregressiveLinear(in_features=5, out_features=5, bias=True, diagonal=0)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoregressiveModel(5, depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution\n",
    "\n",
    "The forward pass will calculate the Bernoulli probability parameter: \n",
    "$$p_i = f(x_1,\\cdots,x_{i-1}).$$ \n",
    "The Bernoulli samples are binary (0 or 1), where $x_i=1$ with probability $p_i$ and $x_i=0$ with probability $1-p_i$. The log conditional probability is given by\n",
    "$$\\log p(x_i|x_1,\\cdots,x_{i-1})= x_i\\log p_i + (1-x_i)\\log(1-p_i).$$\n",
    "The log probability of the autoregressive model is given by the summation\n",
    "$$\\log p(x_1,x_2,\\cdots) = \\sum_{i=1}^n\\log p(x_i|x_1,\\cdots,x_{i-1}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.6187, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoregressiveModel(5, depth=3).log_prob(torch.tensor([0.,1.,0.,0.,0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoregressiveModel(5, depth=3).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Model: Ferromagets\n",
    "\n",
    "Frist generate some fake training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([\n",
    "    [1., 1., 1., 1., 1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model. Initially, the generative model just randomly sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoregressiveModel(5, depth=1)\n",
    "model.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with the dataset use the negative log likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.0002\n",
      "loss : 0.0000\n",
      "loss : 0.0000\n",
      "loss : 0.0000\n",
      "loss : 0.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.5)\n",
    "train_loss = 0.\n",
    "for epoch in range(500):\n",
    "    loss = -torch.sum(model.log_prob(data))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('loss : {:.4f}'.format(train_loss / 100))\n",
    "        train_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns to generate all 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to make the bias very positive. Such that, regardless of what the configuration is, the last layer will always output a probability close to 1, hence the sampler will be strongly biased towards 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3399, -0.1737,  0.1230, -0.2522, -0.1934],\n",
       "         [ 7.7969, -0.1262,  0.2446, -0.0281, -0.0340],\n",
       "         [ 6.2119,  5.8219,  0.3396, -0.2230,  0.0357],\n",
       "         [ 5.4637,  4.9182,  5.5170, -0.1437,  0.1338],\n",
       "         [ 4.9764,  4.4428,  4.5466,  4.7888, -0.1460]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([13.7705,  8.0811,  6.0817,  4.8811,  4.7246], requires_grad=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try a bit more challenging dataset, which are either all-1 or all-0, meaning that the spins are correlated together. The neural network must learn about the correlation to model the dataset correctly. The previous bias trick would not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([\n",
    "    [1., 1., 1., 1., 1.],\n",
    "    [0., 0., 0., 0., 0.]\n",
    "])\n",
    "model = AutoregressiveModel(5, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.5514\n",
      "loss : 1.3880\n",
      "loss : 1.3875\n",
      "loss : 1.3871\n",
      "loss : 1.3869\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1.)\n",
    "train_loss = 0.\n",
    "for epoch in range(500):\n",
    "    loss = -torch.sum(model.log_prob(data))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('loss : {:.4f}'.format(train_loss / 100))\n",
    "        train_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network successfully learns how to generate correlated samples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to make the weight matrix very positive in the lower-triangle to mediate the strong correlation among spins. The first bias vanishes to ensure unbiased sampling between all-1 and all-0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.8387e-01,  5.0940e-02, -2.4698e-01,  2.2660e-01, -1.6552e-02],\n",
       "         [ 1.7140e+01,  2.8545e-01,  3.0627e-01, -3.6814e-02,  9.8888e-02],\n",
       "         [ 9.4619e+00,  9.5402e+00, -4.1717e-01,  3.6980e-01,  3.0317e-02],\n",
       "         [ 7.5032e+00,  8.0729e+00,  7.5546e+00,  2.7915e-01, -8.6019e-02],\n",
       "         [ 6.3904e+00,  6.8154e+00,  6.6266e+00,  6.7329e+00,  1.1359e-01]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 4.1628e-07, -8.3664e+00, -9.1003e+00, -1.1455e+01, -1.1585e+01],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
