{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holographic pixel Graph Convolutional Network (HpGCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lattice System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LatticeSystem(object):\n",
    "    \"\"\" host lattice information and construct graph in hyperbolic space\n",
    "        \n",
    "        Args:\n",
    "        size: number of size along one dimension (assuming square/cubical lattice)\n",
    "        dimension: dimension of the lattice\n",
    "        causal_radius: radius of the causal cone across one level \n",
    "        scale_resolved: whether to distinguish edges from different levels\n",
    "    \"\"\"\n",
    "    def __init__(self, size:int, dimension:int, causal_radius: float = 1., scale_resolved: bool = True):\n",
    "        self.size = size\n",
    "        self.dimension = dimension\n",
    "        self.shape = [size]*dimension\n",
    "        self.sites = size**dimension\n",
    "        self.tree_depth = self.sites.bit_length()\n",
    "        self.node_init()\n",
    "        self.reset_causal_graph(causal_radius, scale_resolved)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'LatticeSystem({} grid with tree depth {}\\n\\t(node_index): {}\\n\\t(edge_index): {}\\n\\t(edge_type): {})'.format('x'.join(str(L) for L in self.shape), self.tree_depth, self.node_index, self.edge_index, self.edge_type)\n",
    "    \n",
    "    def node_init(self):\n",
    "        self.node_levels = torch.zeros(self.sites, dtype=torch.int)\n",
    "        self.node_centers = torch.zeros(self.sites, self.dimension, dtype=torch.float)\n",
    "        self.node_index = torch.zeros(self.sites, dtype=torch.long)\n",
    "        def partition(rng: torch.Tensor, dim: int, ind: int, lev: int):\n",
    "            if rng[dim].sum()%2 == 0:\n",
    "                self.node_levels[ind] = lev\n",
    "                self.node_centers[ind] = rng.to(dtype=torch.float).mean(-1)\n",
    "                mid = rng[dim].sum()//2\n",
    "                rng1 = rng.clone()\n",
    "                rng1[dim, 1] = mid\n",
    "                rng2 = rng.clone()\n",
    "                rng2[dim, 0] = mid\n",
    "                partition(rng1, (dim + 1)%self.dimension, 2*ind, lev+1)\n",
    "                partition(rng2, (dim + 1)%self.dimension, 2*ind + 1, lev+1)\n",
    "            else:\n",
    "                self.node_index[ind-self.sites] = rng[:,0].dot(self.size**torch.arange(0,self.dimension).flip(0))\n",
    "        partition(torch.tensor([[0, self.size]]*self.dimension), 0, 1, 1)\n",
    "        \n",
    "    def reset_causal_graph(self, causal_radius: float, scale_resolved: bool = True):\n",
    "        def discover_causal_connection(z: int):\n",
    "            # Args: z - level of the source\n",
    "            source_pos = self.node_centers[2**(z-1):2**z]\n",
    "            target_pos = self.node_centers[2**z:2**(z+1)]\n",
    "            diff = source_pos.unsqueeze(0) - target_pos.unsqueeze(1)\n",
    "            diff = (diff + self.size/2)%self.size - self.size/2\n",
    "            dist = torch.norm(diff, dim=-1)\n",
    "            smooth_scale = 2**((self.tree_depth-1-z)/self.dimension)\n",
    "            mask = dist < causal_radius * smooth_scale\n",
    "            target_ids, source_ids = torch.nonzero(mask, as_tuple=True)\n",
    "            step_scale = 2**math.floor((self.tree_depth-1-z)/self.dimension)\n",
    "            edge_signatures = torch.round(2*diff/step_scale)[target_ids, source_ids].to(dtype=torch.int)\n",
    "            level_signatures = torch.tensor([[z]]*len(source_ids))\n",
    "            if scale_resolved:\n",
    "                signatures = torch.cat((level_signatures, edge_signatures), -1)\n",
    "            else:\n",
    "                signatures = edge_signatures\n",
    "            return (2**(z-1) + source_ids, 2**z + target_ids, signatures)\n",
    "        level_graded_result = [discover_causal_connection(z) for z in range(1, self.tree_depth-1)]\n",
    "        source_ids, target_ids, signatures = [torch.cat(tens, 0) for tens in zip(*level_graded_result)]\n",
    "        signatures = [tuple(signature) for signature in signatures.tolist()]\n",
    "        distinct_signatures = set(signatures)\n",
    "        self.edge_type_map = {signature: i + 1 for i, signature in enumerate(distinct_signatures)}\n",
    "        self.edge_type = torch.tensor([self.edge_type_map[signature] for signature in signatures])\n",
    "        self.edge_index = torch.stack((source_ids, target_ids), 0)\n",
    "        return self.edge_index, self.edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatticeSystem(4x4 grid with tree depth 5\n",
       "\t(node_index): tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15])\n",
       "\t(edge_index): tensor([[ 1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7],\n",
       "        [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]])\n",
       "\t(edge_type): tensor([1, 5, 6, 3, 6, 3, 4, 2, 4, 2, 4, 2, 4, 2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatticeSystem(4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Space\n",
    "Consider the physical lattice of shape $L\\times\\cdots\\times L = L^d$ of size $L$ in $d$ dimensional space. Each physical site is natually labeled by its real space coordinate $i=(i_0,\\cdots,i_{d-1})$ with each $i_a= 0,\\cdots,L-1$. Instead of labeling sites by coordinate, we can also use the logical index (flattened index)\n",
    "$$i=L^{d-1}i_0+L^{d-2}i_1+\\cdots+L i_{d-2}+i_{d-1}=\\sum_{k=0}^{d-1}L^{d-1-k} i_k.$$\n",
    "Data stored in this order (also known as the C-format) can be viewed as a high dimensional tensor naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H-Tree Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real space lattice is reduced under RG by binary coarse graining along each different axis cyclicly. Under RG the information flows along a H-tree fractal. Following is an example in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/H-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of units:\n",
    "* **site**: physical site on the real space lattice, indexed by $i$ (in red)\n",
    "* **node**: Haar wavelet in the hyperbolic space, indexed by $q$ (in black). Nodes lives on a binary tree (node 0 is not shown in the above figure). The nodes of different **levels** are colored differently. Each node has a **center** position (when it is projected from the holographic bulk to the boundary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LetticeSystem` computes these pieces of information upon initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15]),\n",
       " tensor([0, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32),\n",
       " tensor([[0.0000, 0.0000],\n",
       "         [2.0000, 2.0000],\n",
       "         [1.0000, 2.0000],\n",
       "         [3.0000, 2.0000],\n",
       "         [1.0000, 1.0000],\n",
       "         [1.0000, 3.0000],\n",
       "         [3.0000, 1.0000],\n",
       "         [3.0000, 3.0000],\n",
       "         [0.5000, 1.0000],\n",
       "         [1.5000, 1.0000],\n",
       "         [0.5000, 3.0000],\n",
       "         [1.5000, 3.0000],\n",
       "         [2.5000, 1.0000],\n",
       "         [3.5000, 1.0000],\n",
       "         [2.5000, 3.0000],\n",
       "         [3.5000, 3.0000]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(4, 2)\n",
    "latt.node_index, latt.node_levels, latt.node_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `node_index` is a list of index of sites in the node ordering, such that every RG step corresponds to fusing neighboring nodes.\n",
    "* `node_levels` are levels of nodes.\n",
    "* `node_centers` are center positions of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal Graph Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The H-tree is just a backbone of the hyperbolic space. The actural causal connection can be more extended. The causal relation among nodes forms a **graph**. Two nodes are causally related if the are within a certain radius in the hyperbolic space (the radius can be specified as a hyperparameter). The causal influence always flows from IR to UV in the generative process (so the graph is *directed*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a node $q$, denote its level as $z_q$ and its center as $x_q$. A 0th-order connection is simply a self-loop. A 1st-order connection is from $(z_q,x_q)$ to $(z_{q'}=z_q+1, x_{q'})$ such that $|x_q-x_{q'}|< 2^{\\lfloor (\\zeta-1-z_q)/d\\rfloor}r$, where $r$ is a given radius and $\\zeta=\\log_2 L^d+1$ is the depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes of the same level $z$ are labeled by a contineous range of indices: $q=2^{z-1},\\cdots, 2^z-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "for z in range(1, latt.tree_depth):\n",
    "    print(list(range(2**(z-1), 2**z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through different levels and calculate the rescaled distance, based on which we can pick out the causal connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LatticeSystem` provides the method `reset_causal_graph(radius, scale_resolved=True)` to build/reset the causal graph, given the causality radius. The option `scale_resolved` can be used to switch on or off the scale resolution. The method will return a tuple of edge index and edge type. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7],\n",
       "         [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]]),\n",
       " tensor([1, 5, 6, 3, 6, 3, 4, 2, 4, 2, 4, 2, 4, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(4, 2)\n",
    "latt.reset_causal_graph(1., scale_resolved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7],\n",
       "         [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]]),\n",
       " tensor([3, 4, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(4, 2)\n",
    "latt.reset_causal_graph(1., scale_resolved=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less edge types will be assigned if `scale_resolved` is off, because edges at different scales can now be idendentified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Group` represents a group specified by the multiplication table. Group elements will be labeled by integers (ranging from 0 to the order of the group). The element 0 is always treated as the identity element of the group. `Group` provides methods to perform element-wise group multiplication for Torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    \"\"\"Represent a group, providing multiplication and inverse operation.\n",
    "    \n",
    "    Args:\n",
    "    mul_table: multiplication table as a tensor, e.g. Z2 group: tensor([[0,1],[1,0]])\n",
    "    \"\"\"\n",
    "    def __init__(self, mul_table: torch.Tensor):\n",
    "        super(Group, self).__init__()\n",
    "        self.mul_table = mul_table\n",
    "        self.order = mul_table.size(0) # number of group elements\n",
    "        gs, ginvs = torch.nonzero(self.mul_table == 0, as_tuple=True)\n",
    "        self.inv_table = torch.gather(ginvs, 0, gs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(range(self.order))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Group({} elements)'.format(self.order)\n",
    "    \n",
    "    def inv(self, input: torch.Tensor):\n",
    "        return torch.gather(self.inv_table.expand(input.size()[:-1]+(-1,)), -1, input)\n",
    "    \n",
    "    def mul(self, input1: torch.Tensor, input2: torch.Tensor):\n",
    "        output = input1 * self.order + input2\n",
    "        return torch.gather(self.mul_table.flatten().expand(output.size()[:-1]+(-1,)), -1, output)\n",
    "    \n",
    "    def prod(self, input, dim: int, keepdim: bool = False):\n",
    "        input_size = input.size()\n",
    "        flat_mul_table = self.mul_table.flatten().expand(input_size[:dim]+input_size[dim+1:-1]+(-1,))\n",
    "        output = input.select(dim, 0)\n",
    "        for i in range(1, input.size(dim)):\n",
    "            output = output * self.order + input.select(dim, i)\n",
    "            output = torch.gather(flat_mul_table, -1, output)\n",
    "        if keepdim:\n",
    "            output = output.unsqueeze(dim)\n",
    "        return output\n",
    "    \n",
    "    def val(self, input, val_table = None):\n",
    "        if val_table is None:\n",
    "            val_table = torch.zeros(self.order)\n",
    "            val_table[0] = 1.\n",
    "        elif len(val_table) != self.order:\n",
    "            raise ValueError('Group function value table must be of the same size as the group order, expect {} got {}.'.format(self.order, len(val_table)))\n",
    "        return torch.gather(val_table.expand(input.size()[:-1]+(-1,)), -1, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a $S_3$ group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Group(torch.tensor([[0,1,2,3,4,5],[1,0,3,2,5,4],[2,4,0,5,1,3],[3,5,1,4,0,2],[4,2,5,0,3,1],[5,3,4,1,2,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying two tensors element-wise following the group multiplication rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5],\n",
       "        [1, 2, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0,1,2],[3,4,5]])\n",
    "b = torch.tensor([[5,4,3],[2,1,0]])\n",
    "G.mul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product of each row of a tensor in the given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.prod(a, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group inversion of all elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [4, 3, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.inv(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a group function given by a value table `val_table` (default function: group delta function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.val(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "        [-0.5000, -0.5000,  0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.val(a, val_table=torch.tensor([1.,0.,0.,-0.5,-0.5,0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haar Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lattice system supports Haar tranformation that implements the holographic mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaarTransform(dist.Transform):\n",
    "    \"\"\" Haar wavelet transformation (bijective)\n",
    "        transformation takes real space configurations x to wavelet space encoding y\n",
    "    \n",
    "        Args:\n",
    "        group: a group structure for each unit\n",
    "        lattice: a lattice system containing information of the group and lattice shape\n",
    "    \"\"\"\n",
    "    def __init__(self, group: Group, lattice: LatticeSystem):\n",
    "        super(HaarTransform, self).__init__()\n",
    "        self.group = group\n",
    "        self.lattice = lattice\n",
    "        self.bijective = True\n",
    "        self.make_wavelet()\n",
    "        \n",
    "    # construct Haar wavelet basis\n",
    "    def make_wavelet(self):\n",
    "        self.wavelet = torch.zeros(torch.Size([self.lattice.sites, self.lattice.sites]), dtype=torch.int)\n",
    "        self.wavelet[0] = 1\n",
    "        for z in range(1,self.lattice.tree_depth):\n",
    "            block_size = 2**(z-1)\n",
    "            for q in range(block_size):\n",
    "                node_range = 2**(self.lattice.tree_depth-1-z) * torch.tensor([2*q+1,2*q+2])\n",
    "                nodes = torch.arange(*node_range)\n",
    "                sites = self.lattice.node_index[nodes]\n",
    "                self.wavelet[block_size + q, sites] = 1 \n",
    "                \n",
    "    def _call(self, x):\n",
    "        y = self.group.prod(x.unsqueeze(-1) * self.wavelet, -2)\n",
    "        return y.view(x.size()[:-1]+torch.Size(self.lattice.shape))\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        x = y.flatten(-self.lattice.dimension)[...,self.lattice.node_index]\n",
    "        def renormalize(x):\n",
    "            if x.size(-1) > 1:\n",
    "                x0 = x[...,0::2]\n",
    "                x1 = x[...,1::2]\n",
    "                return torch.cat((renormalize(x0), self.group.mul(self.group.inv(x0), x1)), -1)\n",
    "            else:\n",
    "                return x\n",
    "        return renormalize(x)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation process maps from the holographic bulk (elements on the node $g_q$) to the holographic boundary (elements on the site $f_i$). Starting from $g_q$ on the IR side, in the $z$-th generation step ($z=1,2,\\cdots,\\zeta-1$ where $\\zeta=\\log_2 L^d+1$ is the depth of the tree)\n",
    "$$\\text{for }q=0,\\cdots,2^{z-1}-1, f_{i(2^{\\zeta-1-z}(2q+1:2q+2))} = g_{2^{z-1}+q}f_{i(2^{\\zeta-1-z}(2q+1:2q+2))}.$$\n",
    "The rule can be summerized as a wavelet matrix $w_{iq}=0,1$ such that\n",
    "$$f_i=\\prod_{q}g_q^{w_{iq}},$$\n",
    "where the wavelet matrix is given as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht = HaarTransform(G, LatticeSystem(4, 2))\n",
    "ht.wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The renormalization process maps from the holographic boundary (elements on the site $f_i$) to the holographic bulk (elements on the node $g_q$). The first step is to reshuffle the $g_i$ into a the node order by\n",
    "\n",
    "$$g_q = f_{i(q)},$$\n",
    "\n",
    "where $i(q)$ is the node_index mapping. Now working with $g_q$, in the $z$-th RG step ($z=1,2,\\cdots,\\zeta-1$ where $\\zeta=\\log_2 L^d+1$ is the depth of the RG tree)\n",
    "$$\\text{for }q=0,\\cdots,2^{\\zeta-1-z}-1: g_{q} = g_{2q}, g_{2^{\\zeta-1-z}+q} = g_{2q}^{-1}g_{2q+1}.$$\n",
    "The result of the iteration is the Haar transformed configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 1, 1, 0, 0, 4, 2, 0, 5, 0, 4, 2, 3, 4, 2, 1]]),\n",
       " tensor([[[4, 1, 2, 1],\n",
       "          [4, 4, 1, 3],\n",
       "          [2, 5, 2, 0],\n",
       "          [0, 4, 2, 4]]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(G.order, (1,16))\n",
    "y = ht(x)\n",
    "x, y, ht.inv(y)-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot and Categorical Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample draw from the autoregressive model are one-hot embeddings. Convert between one-hot and categorical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotCategoricalTransform(dist.Transform):\n",
    "    \"\"\"Convert between one-hot and categorical representations.\n",
    "    \n",
    "    Args:\n",
    "    num_classes: number of classes.\"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(OneHotCategoricalTransform, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.bijective = True\n",
    "    \n",
    "    def _call(self, x):\n",
    "        # one-hot to categorical\n",
    "        return x.max(dim=-1)[1]\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        # categorical to one-hot\n",
    "        return F.one_hot(y, self.num_classes).to(dtype=torch.float)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1],\n",
       "        [1, 1, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(2, (2,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc = OneHotCategoricalTransform(2)\n",
    "y = oc.inv(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1],\n",
       "        [1, 1, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be pack into a Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GCN` uses `torch_geometric.nn.MessagePassing` to create a graph convolution network of given numbers of input features $f_\\text{in}$, output features $f_\\text{out}$ and edge features $f_\\text{edge}$. \n",
    "\n",
    "**Input**: \n",
    "* `x`: node feature vectors $x_q$.\n",
    "* `edge_index`: a tensor of shape $(2,E)$ where $E$ is the number of (active) edges in the graph. `edge_index[0]` gives the source node indices, `edge_index[1]` gives the target node indices correspondingly. Together they specify a directed graph (the causal influence graph).\n",
    "* `edge_attr`: edge attribution (edge feature vectors) $e_{pq}$.\n",
    "\n",
    "**output**:\n",
    "* `y`: a new set of node feature vectors $y_p$, given by\n",
    "$$y_p = \\sum_{q\\in\\mathcal{N}(p)} e_{pq}^\\intercal (W x_q + b),$$\n",
    "where $W$ is a rank-3 tensor of the shape $(f_\\text{edge}, f_\\text{out}, f_\\text{in})$ and $b$ is a matrix of the shape $(f_\\text{edge}, f_\\text{out})$. The additive bias $b$ can be turned off by setting `bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "class GraphConv(MessagePassing):\n",
    "    \"\"\" Graph Convolution layer \n",
    "        \n",
    "        Args:\n",
    "        in_features: number of input features per node\n",
    "        out_features: number of output features per node \n",
    "        edge_features: number of features per edge\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, edge_features: int, bias: bool = True):\n",
    "        super(GraphConv, self).__init__(aggr='add')\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.edge_features = edge_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(edge_features, out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(edge_features, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, edge_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.edge_features, self.bias is not None)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        bound = 1 / math.sqrt(self.weight.size(1))\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: shape [..., N, in_features]\n",
    "        # edge_index: shape [2, E]\n",
    "        # edge_attr: shape [E, edge_features]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "    \n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: shape [..., E, in_features]\n",
    "        # edge_attr: [E, edge_features]\n",
    "        weight = torch.tensordot(edge_attr, self.weight, dims=1)\n",
    "        x_j = torch.sum(weight * x_j.unsqueeze(-2), -1)\n",
    "        if self.bias is not None:\n",
    "            bias = torch.tensordot(edge_attr, self.bias, dims=1)\n",
    "            x_j += bias\n",
    "        return x_j\n",
    "    \n",
    "    def forward_from(self, x, i, edge_index, edge_attr):\n",
    "        mask = (edge_index[0] == i)\n",
    "        return self(x, edge_index[:, mask], edge_attr[mask, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphConv(in_features=2, out_features=3, edge_features=3, bias=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc = GraphConv(2, 3, 3)\n",
    "gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0559,  1.3136,  0.3057],\n",
       "         [-0.6175, -0.1689,  0.4042]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0559,  1.3136,  0.3057],\n",
       "         [-0.0549,  0.1292,  0.5020]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0559,  1.3136,  0.3057],\n",
       "         [-0.6175, -0.1689,  0.4042]]], grad_fn=<ScatterAddBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "        [[[0., 1.],\n",
    "          [1., 0.],\n",
    "          [0., 1.],\n",
    "          [1., 0.]],\n",
    " \n",
    "         [[1., 0.],\n",
    "          [1., 0.],\n",
    "          [1., 0.],\n",
    "          [0., 1.]],\n",
    " \n",
    "         [[1., 0.],\n",
    "          [1., 0.],\n",
    "          [0., 1.],\n",
    "          [1., 0.]]])\n",
    "edge_index = torch.tensor([[1, 1, 2],\n",
    "                           [2, 3, 3]])\n",
    "edge_attr = torch.tensor([[-0.9325, -1.0142,  1.5556],\n",
    "                          [-0.7909, -0.4774, -1.2065],\n",
    "                          [-0.1908, -0.6673,  0.1268]])\n",
    "gc(x, edge_index, edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be used in the autoregressive model, `GraphConv` also provides a `forward_from` method that allows to forward pass from a single given node. This is implemented by masking out other edges that are not going out from the specific node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index[0] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.2210,  0.2515,  0.1008]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.3416,  0.5496,  0.1987]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.2210,  0.2515,  0.1008]]], grad_fn=<ScatterAddBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.forward_from(x, 2, edge_index, edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoregressiveModel` uses graph convolutional network (GCN) to model the conditional probability distribution following the causal influence (message passing on a directed graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure (a) shows the causal graph of 8 nodes in a binary tree. The node 0  is somewhat special, that it is always sampled independently from the uniform distribution, and it does not cast causal influence to other nodes. Edges are colored differently to indicate different types of causal influences. Figure (b) is the multi-layer GCN that respects the causal relation. It is important that the first (bottom) layer should not have self-loop connections, but the remaining layers can have. Edges of different colors will correspond to different linear maps of the node feature vectors. The edge type is first maped to a edge feature vector by an embedding layer, the edge feature vectors are then distributed to and shared by the GCN layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierachical autoregressive model models the probability of a sample $z$ as\n",
    "$$\\begin{split}\n",
    "p(z)=&p(z_0)p(z_1)p(z_2|z_1)p(z_3|z_1)\\\\\n",
    "&p(z_4|z_1,z_2)p(z_5|z_1,z_2)p(z_6|z_1,z_3)p(z_7|z_1,z_3)\n",
    "\\end{split}$$\n",
    "The conditional distributions are modeled by neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the advange of modeling the Haar encoding in the holographic space?\n",
    "* **Resolve the criticality**: the holographic mapping brings a scale-free system to a local system (with an emergent scale set by the hyperbolic radius and the critical exponent). This can be seen from the correlation function \n",
    "$$C(r)\\sim r^{-\\alpha}\\to \\sim e^{-d/\\xi},$$\n",
    "where $d=R\\ln r$ and $\\xi = R/\\alpha$. The complexity of modeling correlation at all scales on the boundary is reduced to modeling correlations locally in the bulk.\n",
    "* **Shorten the causal chain**: conventional approach like pixel-CNN has unnatural causal structures (why a single pixel must causally depend on its upper-half-plane?). The natural way to think about generating a image is to follow the reverse process of renormalization group. The scale itself becomes the emergent time direction in the hyperbolic space which defines a more natural causal structure: start paining the outline first, then add the details. A remarkable feature is that *time is short in the holographic bulk*, the causal chain is at most of the length $\\sim\\log L$ (i.e. logarithmic in system size), and the causal cone has limited width (like the past light cone in an expanding universe, which light can not catch up the collapse of universe if we look backwards). This makes the sampling and generation efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveModel(nn.Module, dist.Distribution):\n",
    "    \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n",
    "        \n",
    "        Args:\n",
    "        nodes: number of units in the model\n",
    "        features: a list of feature dimensions from the input layer to the output layer\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the additive bias in heap linear layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lattice: LatticeSystem, edge_features: int, node_features, \n",
    "                 nonlinearity: str = 'ReLU', bias: bool = True):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "        self.lattice = lattice\n",
    "        self.nodes = self.lattice.sites\n",
    "        self.edge_index = self.lattice.edge_index\n",
    "        self.edge_type = self.lattice.edge_type\n",
    "        self.edge_index_ext, self.edge_type_ext = self.edge_extension()\n",
    "        self.num_edge_type = self.edge_type.max() + 1\n",
    "        self.edge_features = edge_features\n",
    "        self.edge_embedding = nn.Embedding(self.num_edge_type, self.edge_features)\n",
    "        if isinstance(node_features, int):\n",
    "            self.node_features = [node_features, node_features]\n",
    "        else:\n",
    "            if node_features[0] != node_features[-1]:\n",
    "                raise ValueError('In features {}, the first and last feature dimensions must be equal.'.format(features))\n",
    "            self.node_features = node_features\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in range(1, len(self.node_features)):\n",
    "            if l > 1: \n",
    "                self.layers.append(getattr(nn, nonlinearity)())\n",
    "            self.layers.append(GraphConv(self.node_features[l - 1], self.node_features[l], self.edge_features, bias))\n",
    "        dist.Distribution.__init__(self, event_shape=torch.Size([self.nodes, self.node_features[0]]))\n",
    "        self.has_rsample = True\n",
    "    \n",
    "    def edge_extension(self):\n",
    "        node_list = torch.arange(1, self.nodes)\n",
    "        edge_index = torch.stack((node_list, node_list), 0)\n",
    "        edge_type = torch.zeros(edge_index.size(-1), dtype=self.edge_type.dtype)\n",
    "        edge_index_ext = torch.cat((edge_index, self.edge_index), -1)\n",
    "        edge_type_ext = torch.cat((edge_type, self.edge_type), -1)\n",
    "        return edge_index_ext, edge_type_ext\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return '(nodes): {}, (edge_features): {}, (node_features): {}'.format(self.nodes, self.edge_features, self.node_features) + super(AutoregressiveModel, self).extra_repr()\n",
    "          \n",
    "    def forward(self, input):\n",
    "        edge_attr = self.edge_embedding(self.edge_type)\n",
    "        edge_attr_ext = self.edge_embedding(self.edge_type_ext)\n",
    "        for l, layer in enumerate(self.layers): # apply layers\n",
    "            if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                if l == 0: # first layer\n",
    "                    output = layer(input, self.edge_index, edge_attr)\n",
    "                else: # remaining layers\n",
    "                    output = layer(output, self.edge_index_ext, edge_attr_ext)\n",
    "            else: # activation layers\n",
    "                output = layer(output)\n",
    "        return output # logits\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        logits = self(value) # forward pass to get logits\n",
    "        return torch.sum(value * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "\n",
    "    def sampler(self, logits, dim=-1): # simplified from F.gumbel_softmax\n",
    "        gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "        gumbels += logits\n",
    "        index = gumbels.max(dim, keepdim=True)[1]\n",
    "        return torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "\n",
    "    def _sample(self, batch_size: int, sampler = None):\n",
    "        if sampler is None: # use default sampler\n",
    "            sampler = self.sampler\n",
    "        # create a list of tensors to cache layer-wise outputs\n",
    "        cache = [torch.zeros(batch_size, self.nodes, self.node_features[0])]\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                node_features = layer.out_features\n",
    "                cache.append(torch.zeros(batch_size, self.nodes, node_features))\n",
    "            else: # activation layers\n",
    "                cache.append(torch.zeros(batch_size, self.nodes, node_features))\n",
    "        # autoregressive batch sampling\n",
    "        edge_attr = self.edge_embedding(self.edge_type)\n",
    "        edge_attr_ext = self.edge_embedding(self.edge_type_ext)\n",
    "        cache[0][..., 0, :] = sampler(cache[0][..., 0, :]) # always sample node 0 uniformly\n",
    "        for i in range(1, self.nodes):\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                    if l==0: # first layer\n",
    "                        cache[l + 1] += layer.forward_from(cache[l], i - 1, self.edge_index, edge_attr)\n",
    "                    else: # remaining layers\n",
    "                        cache[l + 1] += layer.forward_from(cache[l], i, self.edge_index_ext, edge_attr_ext)\n",
    "                else: # activation layers\n",
    "                    cache[l + 1][..., i, :] = layer(cache[l][..., i, :])\n",
    "            # the last cache hosts the logit, sample from it \n",
    "            cache[0][..., i, :] = sampler(cache[-1][..., i, :])\n",
    "        return cache # cache[0] hosts the sample\n",
    "    \n",
    "    def sample(self, batch_size=1):\n",
    "        with torch.no_grad():\n",
    "            cache = self._sample(batch_size)\n",
    "        return cache[0]\n",
    "    \n",
    "    def rsample(self, batch_size=1, tau=None, hard=False):\n",
    "        if tau is None: # if temperature not given\n",
    "            tau = 1/(self.features[-1]-1) # set by the out feature dimension\n",
    "        cache = self._sample(batch_size, lambda x: F.gumbel_softmax(x, tau, hard))\n",
    "        return cache[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: create a hierachical autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "latt = LatticeSystem(4, 2)\n",
    "ar = AutoregressiveModel(latt, 3, [G.order, 3, G.order])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains the following components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoregressiveModel(\n",
       "  (nodes): 16, (edge_features): 3, (node_features): [2, 3, 2]\n",
       "  (edge_embedding): Embedding(7, 3)\n",
       "  (layers): ModuleList(\n",
       "    (0): GraphConv(in_features=2, out_features=3, edge_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): GraphConv(in_features=3, out_features=2, edge_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the forward pass recovers the logits generated through the autoregressive sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache = ar._sample(1)\n",
    "ar(cache[0]) - cache[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need an energy model to describe the Statistical Mechanics system. `EnergyModel` provides the function to evalutate the energy of a configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phys import *\n",
    "class EnergyModel(nn.Module):\n",
    "    \"\"\" Energy mdoel that describes the physical system. Provides function to evaluate energy.\n",
    "    \n",
    "        Args:\n",
    "        group: a specifying the group on each site\n",
    "        lattice: a lattice system containing information of the group and lattice shape\n",
    "        energy: lattice Hamiltonian in terms of energy terms\n",
    "    \"\"\"\n",
    "    def __init__(self, group: Group, lattice: LatticeSystem, energy: EnergyTerms):\n",
    "        super(EnergyModel, self).__init__()\n",
    "        self.group = group\n",
    "        self.lattice = lattice\n",
    "        self.energy = energy.on(self.group, self.lattice)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return '(lattice): {}'.format(self.lattice) + super(EnergyModel, self).extra_repr()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.energy(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 2D Ising model on a square lattice\n",
    "$$H= -J \\sum_{i}(\\sigma_i\\sigma_{i+\\hat{x}} + \\sigma_i\\sigma_{i+\\hat{y}}).$$\n",
    "The Hamiltonian can be typed in as (see the following subsection for explaination of the notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 0.7\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.], dtype=float), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.], dtype=float), (0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy model is defined by the lattice system and the Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyModel(\n",
       "  (lattice): LatticeSystem(4x4 grid with tree depth 5\n",
       "  \t(node_index): tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15])\n",
       "  \t(edge_index): tensor([[ 1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7],\n",
       "          [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]])\n",
       "  \t(edge_type): tensor([1, 5, 6, 3, 6, 3, 4, 2, 4, 2, 4, 2, 4, 2]))\n",
       "  (energy): EnergyTerms(\n",
       "    (0): TwoBody(tensor([-0.7000,  0.7000], dtype=torch.float64) across (1, 0))\n",
       "    (1): TwoBody(tensor([-0.7000,  0.7000], dtype=torch.float64) across (0, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "latt = LatticeSystem(4, 2)\n",
    "energy = EnergyModel(G, latt, H)\n",
    "energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some spin configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 1, 0],\n",
       "         [0, 1, 1, 0],\n",
       "         [1, 1, 0, 1],\n",
       "         [0, 1, 0, 1]],\n",
       "\n",
       "        [[1, 0, 1, 1],\n",
       "         [0, 1, 0, 0],\n",
       "         [1, 0, 1, 1],\n",
       "         [0, 1, 1, 1]],\n",
       "\n",
       "        [[0, 1, 1, 0],\n",
       "         [1, 0, 1, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [1, 0, 1, 1]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = AutoregressiveModel(latt, 3, [G.order, 3, G.order], bias=False)\n",
    "oc = OneHotCategoricalTransform(G.order)\n",
    "ht = HaarTransform(G, latt)\n",
    "x = ht(oc(ar.sample(3)))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy model can be used to evalutate the energy of these spin configuraitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.4000, 5.6000, 2.8000], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hamiltonian Scripting System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(See [phys.py](./phys.py) for code details).\n",
    "\n",
    "In order to facilitate the intuitive formulation of Hamiltonian, we have introduced a scripting system. Physical Hamiltonians are always sum of local energy terms. In this system, each energy term is a subclass of `nn.Module` and each Hamiltonian is a subclass of `nn.ModuleList` (which contains the collection of energy terms). In this way, the evaluation of the total energy of the Hamiltonian can be passed down to each energy terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced two kinds of energy terms\n",
    "* `OnSite`: on-site energy term $E_1(g_i)$,\n",
    "* `TwoBody`: two-body interaction term $E_2(g_i,g_j)$.\n",
    "\n",
    "More complicated interaction terms can be introduced under this framework if necessary. These energy terms are group functions: $E_1:G\\to\\mathbb{R}$, $E_2:G\\times G\\to\\mathbb{R}$. These group functions can be specified by a value table, which enumerated the value that each group element maps to. For example, for the $\\mathbb{Z}_2=\\{0,1\\}$ group ($0$-identity, $1$-generator), if we want to specify $E_1(g_\\sigma)=\\sigma$, i.e.\n",
    "$$E_1(0)=+1, E_1(1)=-1,$$\n",
    "the value talbe is $[+1,-1]$. Such a term can be created as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OnSite(tensor([ 1., -1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnSite(torch.tensor([1.,-1.],dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-body term, we assume that it always take the form of\n",
    "$$E_2(g_i,g_j)=E_2(g_i^{-1}g_j),$$\n",
    "such that we will only need to a single-variable group function, unsing the same value table representation. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoBody(tensor([ 1., -1.], dtype=torch.float64) across (1, 0))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwoBody(torch.tensor([1.,-1.],dtype=float), (1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-body term also carries a second argument to specify the relative direction from site-$i$ to site-$j$. If the value table is not specified, the default group function will be taken to be the delta function (like Potts model), which maps the identity element to 1 and the others to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this system, we can add, subtract, scalar multiply and negate the energy terms. Energy terms adding together will be represented as a collection of terms in a list (`nn.ModuleList`), which correspond to a Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyTerms(\n",
       "  (0): TwoBody(5.2)\n",
       "  (1): TwoBody(5.2)\n",
       "  (2): OnSite(-2.8)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.8 * OnSite() + 5.2 * (TwoBody(shifts=(1,0)) + TwoBody(shifts=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Hamiltonian `H` needs to be further put on a lattice by calling `H.on(lattice)`. Only after it putting on a lattice, the Hamiltonian has a concrete meaning, and then the energy of the system can be evaluated by the `H.forward(input)` method on a spin configuration as `input`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holographic pixel Graph Convolutional Network (HpGCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all components together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HpGCN(nn.Module, dist.TransformedDistribution):\n",
    "    \"\"\" Combination of hierarchical autoregressive and flow-based model for lattice models.\n",
    "    \n",
    "        Args:\n",
    "        energy: a energy model to learn\n",
    "        hidden_features: a list of feature dimensions of hidden layers\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the additive bias in heap linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, energy: EnergyModel, edge_features: int, hidden_node_features):\n",
    "        super(HpGCN, self).__init__()\n",
    "        self.energy = energy\n",
    "        self.group = energy.group\n",
    "        self.lattice = energy.lattice\n",
    "        self.haar = HaarTransform(self.group, self.lattice)\n",
    "        self.onecat = OneHotCategoricalTransform(self.group.order)\n",
    "        node_features = [self.group.order] + hidden_node_features + [self.group.order]\n",
    "        auto = AutoregressiveModel(self.lattice, edge_features, node_features)\n",
    "        dist.TransformedDistribution.__init__(self, auto, [self.onecat, self.haar])\n",
    "        self.transform = dist.ComposeTransform(self.transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a holographic pixel flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HpGCN(\n",
       "  (energy): EnergyModel(\n",
       "    (lattice): LatticeSystem(4x4 grid with tree depth 5\n",
       "    \t(node_index): tensor([ 0,  1,  4,  5,  2,  3,  6,  7,  8,  9, 12, 13, 10, 11, 14, 15])\n",
       "    \t(edge_index): tensor([[ 1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7],\n",
       "            [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]])\n",
       "    \t(edge_type): tensor([1, 5, 6, 3, 6, 3, 4, 2, 4, 2, 4, 2, 4, 2]))\n",
       "    (energy): EnergyTerms(\n",
       "      (0): TwoBody(tensor([-0.7000,  0.7000]) across (1, 0))\n",
       "      (1): TwoBody(tensor([-0.7000,  0.7000]) across (0, 1))\n",
       "    )\n",
       "  )\n",
       "  (base_dist): AutoregressiveModel(\n",
       "    (nodes): 16, (edge_features): 3, (node_features): [2, 3, 2]\n",
       "    (edge_embedding): Sequential(\n",
       "      (0): Embedding(7, 3)\n",
       "      (1): Softmax(dim=-1)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphConv(in_features=2, out_features=3, edge_features=3, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): GraphConv(in_features=3, out_features=2, edge_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run \"main.py\"\n",
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "latt = LatticeSystem(4, 2)\n",
    "J = 0.7\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "model = HpGCN(EnergyModel(G, latt, H), 3, [3])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw samples from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 0],\n",
       "         [1, 0, 1, 1],\n",
       "         [0, 1, 0, 1],\n",
       "         [1, 1, 0, 0]],\n",
       "\n",
       "        [[1, 1, 0, 0],\n",
       "         [1, 1, 1, 0],\n",
       "         [0, 0, 0, 1],\n",
       "         [1, 0, 1, 0]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.sample(2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate log probabilities of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15.0476, -11.3792], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.log_prob(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate energies of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8000, 5.6000])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.energy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform samples to the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform.inv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reverse KL with log-trick**. The goal is to minimize the difference between the model distribution $q_\\theta(x)$ and the target distribution $p(x) \\propto e^{-E(x)}$ by minimizing the reverse KL divergence\n",
    "$$\\begin{split}\\mathcal{L}&=\\mathsf{KL}(q_\\theta||p)\\\\\n",
    "&=\\sum_{x} q_\\theta(x) \\ln \\frac{q_\\theta(x)}{p(x)}\\\\\n",
    "&=\\sum_{x}q_\\theta(x)(E(x)+\\ln q_\\theta(x)). \n",
    "\\end{split}$$\n",
    "The parameter dependence is only in $q_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is given by\n",
    "$$\\begin{split}\\partial_\\theta\\mathcal{L}&= \\partial_\\theta \\sum_{x}q_\\theta(x)(E(x)+\\ln q_\\theta(x))\\\\\n",
    "&= \\sum_{x}[(\\partial_\\theta q_\\theta(x))(E(x)+\\ln q_\\theta(x))+q_\\theta(x)\\partial_\\theta \\ln q_\\theta(x)]\\\\\n",
    "\\end{split}$$\n",
    "The last term can be dropped because \n",
    "$$\\sum_x q_\\theta(x)\\partial_\\theta \\ln q_\\theta(x) = \\sum_x \\partial_\\theta q_\\theta(x)=\\partial_\\theta\\sum_x q_\\theta(x)=\\partial_\\theta 1 = 0,$$\n",
    "the remaining term reads\n",
    "$$\\begin{split}\\partial_\\theta\\mathcal{L}&= \\sum_{x}(\\partial_\\theta q_\\theta(x))(E(x)+\\ln q_\\theta(x))\\\\\n",
    "&= \\sum_{x}(\\partial_\\theta q_\\theta(x))R(x)\\\\\n",
    "&= \\mathbb{E}_{x\\sim q_\\theta}(\\partial_\\theta \\ln q_\\theta(x))R(x)\\\\\n",
    "\\end{split}$$\n",
    "with a reward signal $R(x)=E(x)+\\ln q_\\theta(x)$ in the context of reinforcement learning. The gradient signal $\\partial_\\theta \\ln q_\\theta(x)$ is weighted by $R(x)$, such that when $R(x)$ is large for a configuration $x$, the gradient descent will decrease the log likelihood $\\ln q_\\theta(x)$ for that configuration, hence the optimzation will try to reduce the free energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we should not just drop the last term for finite batches, instead we should introduce a Lagrangian multiplier to counter the gradient signal that is towards the direction of violating the normalization condition. This amounts to subtracting $R(x)$ by a baseline value $b=\\mathbb{E}_{x\\sim q_\\theta} R(x)$, which can be estimated within each batch. The baseline subtraction helps to reduce the variance of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"main.py\"\n",
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "latt = LatticeSystem(2, 2, 1.)\n",
    "J = 0.1\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "model = HpGCN(EnergyModel(G, latt, H), 5, [3], nonlinearity='ReLU' , bias=True)\n",
    "#model.base_dist.lattice.edge_type.fill_(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4743, free energy: -5.0326\n",
      "loss: -1.2254, free energy: -5.0419\n",
      "loss: -0.5264, free energy: -5.0880\n",
      "loss: -0.1949, free energy: -5.0958\n",
      "loss: 1.9242, free energy: -5.0534\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "free_energy = 0.\n",
    "echo = 100\n",
    "for epoch in range(500):\n",
    "    x = model.sample(batch_size)\n",
    "    log_prob = model.log_prob(x)\n",
    "    energy = model.energy(x)\n",
    "    free = energy + log_prob.detach()\n",
    "    meanfree = free.mean()\n",
    "    loss = torch.sum(log_prob * (free - meanfree))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    free_energy += meanfree.item()\n",
    "    if (epoch+1)%echo == 0:\n",
    "        print('loss: {:.4f}, free energy: {:.4f}'.format(train_loss/echo, free_energy/echo))\n",
    "        train_loss = 0.\n",
    "        free_energy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyModel(\n",
       "  (lattice): LatticeSystem(2x2 grid with tree depth 3\n",
       "  \t(node_index): tensor([0, 1, 2, 3])\n",
       "  \t(edge_index): tensor([[1, 1],\n",
       "          [2, 3]])\n",
       "  \t(edge_type): tensor([1, 2]))\n",
       "  (energy): EnergyTerms(\n",
       "    (0): TwoBody(tensor([-0.7000,  0.7000]) across (1, 0))\n",
       "    (1): TwoBody(tensor([-0.7000,  0.7000]) across (0, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = 0.7\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "model.energy.update(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.411565363407135\n",
      "1 0.028368087485432625\n",
      "2 0.0018196612363681197\n",
      "3 0.026399722322821617\n",
      "4 0.02799769677221775\n",
      "5 0.0019298052648082376\n",
      "6 0.00012378675455693156\n",
      "7 0.0017959026154130697\n",
      "8 0.0017959026154130697\n",
      "9 0.00012378675455693156\n",
      "10 0.0019298052648082376\n",
      "11 0.02799769677221775\n",
      "12 0.026399722322821617\n",
      "13 0.0018196612363681197\n",
      "14 0.028368087485432625\n",
      "15 0.411565363407135\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "xs = torch.tensor(list(itertools.product([0,1],repeat=4))).view(-1,2,2)\n",
    "with torch.no_grad():\n",
    "    ps = model.log_prob(xs).exp()\n",
    "for i, p in enumerate(ps):\n",
    "    print(i, p.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is not stable. Possible reason:\n",
    "* Compared with the previous version, the UI/IR mixing is reduced. Increasing the pressure on the hidden layer.\n",
    "* The link variety increases, this makes the model harder to converge.\n",
    "* Easily trapped at local minimum when temperature is low (try annealing?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [1, 1]])\n",
      "tensor(-3.6344, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "opt2 = optim.SGD(model.parameters(), lr=0.5)\n",
    "x = xs[3]\n",
    "print(x)\n",
    "opt2.zero_grad()\n",
    "obj = model.log_prob(xs[3])\n",
    "print(obj)\n",
    "obj.backward()\n",
    "opt2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0522,  0.0451,  0.0179,  0.0295, -0.1447],\n",
      "        [ 0.0026, -0.0071,  0.0008,  0.0020,  0.0016],\n",
      "        [ 0.0018, -0.0065,  0.0010,  0.0012,  0.0025]])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[-0.0197,  0.0197],\n",
      "        [ 0.0994, -0.0994],\n",
      "        [-0.0022,  0.0022],\n",
      "        [-0.2875,  0.2875],\n",
      "        [-0.4733,  0.4733]])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is very sparse. There are only very limited ways to tune the paramters. Model is too rigid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Problems**\n",
    "* There is no node specific bias!\n",
    "* There is no three body interaction to capture the correlation of UV nodes conditioned on IR\n",
    "* There is no regularization of parameters, parameters could do random walk. (But this is simple to solve)\n",
    "\n",
    "**Future Plan**\n",
    "* Develop a geometric system, which allows us to systematically capture all the simplicial objects within the local causal range. Instead trying to assign features to edges faces and higher objects, we just need to introduce an embedding for every node, and derive the higher-object feature vector by aggregation (presumably realized by an RNN?). The simplest position encoding for a node is the plan-wave encoding in the hypobolic space (we want to preserve the translation symmetry). Such a feature can be either trainable or not. We hope that translationally equivalent edges and faces will have the same feature, such that parameters can be shared effectively. Then We can use the edge and face features to build a conditional distribution. It always takes the form of $p(x_i|x_{j<i})$. We will sum up the contributions of all higher objects together on the log likelihood level to made a final probability model that is normalized and not over counting.\n",
    "* Maybe higher object is not really necessary, we just need more edges? Given that attention is all you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Causal Graph + Node Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"main.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lattice System Upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade lattice system to provide infomation about extended causal graph and node position encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LatticeSystem(object):\n",
    "    \"\"\" host lattice information and construct graph in hyperbolic space\n",
    "        \n",
    "        Args:\n",
    "        size: number of size along one dimension (assuming square/cubical lattice)\n",
    "        dimension: dimension of the lattice\n",
    "        causal_radius: radius of the causal cone across one level \n",
    "        scale_resolved: whether to distinguish edges from different levels\n",
    "    \"\"\"\n",
    "    def __init__(self, size:int, dimension:int):\n",
    "        self.size = size\n",
    "        self.dimension = dimension\n",
    "        self.shape = [size]*dimension\n",
    "        self.sites = size**dimension\n",
    "        self.tree_depth = self.sites.bit_length()\n",
    "        self.node_init()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'LatticeSystem({} grid with tree depth {})'.format('x'.join(str(L) for L in self.shape), self.tree_depth)\n",
    "    \n",
    "    def node_init(self):\n",
    "        self.node_levels = torch.zeros(self.sites, dtype=torch.int)\n",
    "        self.node_centers = torch.zeros(self.sites, self.dimension, dtype=torch.float)\n",
    "        self.node_index = torch.zeros(self.sites, dtype=torch.long)\n",
    "        def partition(rng: torch.Tensor, dim: int, ind: int, lev: int):\n",
    "            if rng[dim].sum()%2 == 0:\n",
    "                self.node_levels[ind] = lev\n",
    "                self.node_centers[ind] = rng.to(dtype=torch.float).mean(-1)\n",
    "                mid = rng[dim].sum()//2\n",
    "                rng1 = rng.clone()\n",
    "                rng1[dim, 1] = mid\n",
    "                rng2 = rng.clone()\n",
    "                rng2[dim, 0] = mid\n",
    "                partition(rng1, (dim + 1)%self.dimension, 2*ind, lev+1)\n",
    "                partition(rng2, (dim + 1)%self.dimension, 2*ind + 1, lev+1)\n",
    "            else:\n",
    "                self.node_index[ind-self.sites] = rng[:,0].dot(self.size**torch.arange(0,self.dimension).flip(0))\n",
    "        partition(torch.tensor([[0, self.size]]*self.dimension), 0, 1, 1)\n",
    "        \n",
    "    def causal_graph(self, causal_radius: float = 1.):\n",
    "        def discover_causal_connection(z: int):\n",
    "            # Args: z - level of the source\n",
    "            source_pos = self.node_centers[2**(z-1):2**z]\n",
    "            target_pos = self.node_centers[2**z:2**(z+1)]\n",
    "            diff = source_pos.unsqueeze(0) - target_pos.unsqueeze(1) # difference \n",
    "            diff = (diff + self.size/2)%self.size - self.size/2 # assuming periodic boundary\n",
    "            dist = torch.norm(diff, dim=-1) # distance\n",
    "            smooth_scale = 2**((self.tree_depth-1-z)/self.dimension)\n",
    "            mask = dist < causal_radius * smooth_scale\n",
    "            target_ids, source_ids = torch.nonzero(mask, as_tuple=True)\n",
    "            return (2**(z-1) + source_ids, 2**z + target_ids)\n",
    "        level_graded_result = [discover_causal_connection(z) for z in range(1, self.tree_depth-1)]\n",
    "        # nearest layer causal connections within radius\n",
    "        adj1 = self.to_adj(torch.stack([torch.cat(tens, 0) for tens in zip(*level_graded_result)]))\n",
    "        adj2 = adj1 @ adj1\n",
    "        #adj3 = adj2 @ adj1\n",
    "        adj11 = torch.tril(adj1 @ adj1.t(), -1)\n",
    "        adj22 = torch.tril(adj2 @ adj2.t(), -1) # overlap adj11\n",
    "        adj21 = torch.tril(adj2 @ adj1.t(), -1) # overlap adj1\n",
    "        edge_index = torch.cat([self.to_edge_index(adj) for adj in \n",
    "                                [adj1 + adj21, adj11 + adj22, adj2]], -1)\n",
    "        return edge_index\n",
    "    \n",
    "    def to_adj(self, edge_index):\n",
    "        ones = torch.ones(edge_index.size(-1), dtype=torch.long)\n",
    "        return torch.sparse.LongTensor(edge_index.flip(0), ones, torch.Size([self.sites]*2)).to_dense()\n",
    "    \n",
    "    def to_edge_index(self, adj):\n",
    "        target, source = torch.nonzero(adj, as_tuple=True)\n",
    "        return torch.stack([source, target])\n",
    "    \n",
    "    def node_position_encoding(self, node_features: int = None):\n",
    "        phase = 2*math.pi*self.node_centers/self.size\n",
    "        levels = self.node_levels.to(dtype=torch.float).unsqueeze(-1)\n",
    "        encoding = torch.cat([phase.sin(), phase.cos(), levels], -1)\n",
    "        if node_features is not None:\n",
    "            pad = node_features - encoding.size(-1)\n",
    "            if pad < 0:\n",
    "                raise RuntimeError('Number of node features must be no less than 5, got {}.'.format(node_features))\n",
    "            zeros = torch.zeros(self.sites, pad)\n",
    "            encoding = torch.cat([encoding, zeros], -1)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend causal relations to include more local relations in the hyperbolic space. \n",
    "* 亲子关系\n",
    "* 婶侄关系\n",
    "* 姐弟关系（亲+表）\n",
    "* 祖孙关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  2,  3,  2,  3,  2,  3,  2,  3,  4,  5,  4,  5,  4,  5,  4,  5,\n",
       "          6,  7,  6,  7,  6,  7,  6,  7,  2,  4,  4,  5,  4,  5,  6,  8,  8,  9,\n",
       "          8,  9, 10, 12, 12, 13, 12, 13, 14,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n",
       "          3,  3,  3],\n",
       "        [ 2,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9, 10, 10, 11, 11,\n",
       "         12, 12, 13, 13, 14, 14, 15, 15,  3,  5,  6,  6,  7,  7,  7,  9, 10, 10,\n",
       "         11, 11, 11, 13, 14, 14, 15, 15, 15,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(4, 2)\n",
    "latt.causal_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use node position encoding to initialize node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00],\n",
       "        [-8.7423e-08, -8.7423e-08, -1.0000e+00, -1.0000e+00,  1.0000e+00],\n",
       "        [ 1.0000e+00, -8.7423e-08, -4.3711e-08, -1.0000e+00,  2.0000e+00],\n",
       "        [-1.0000e+00, -8.7423e-08,  1.1925e-08, -1.0000e+00,  2.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00, -4.3711e-08, -4.3711e-08,  3.0000e+00],\n",
       "        [ 1.0000e+00, -1.0000e+00, -4.3711e-08,  1.1925e-08,  3.0000e+00],\n",
       "        [-1.0000e+00,  1.0000e+00,  1.1925e-08, -4.3711e-08,  3.0000e+00],\n",
       "        [-1.0000e+00, -1.0000e+00,  1.1925e-08,  1.1925e-08,  3.0000e+00],\n",
       "        [ 7.0711e-01,  1.0000e+00,  7.0711e-01, -4.3711e-08,  4.0000e+00],\n",
       "        [ 7.0711e-01,  1.0000e+00, -7.0711e-01, -4.3711e-08,  4.0000e+00],\n",
       "        [ 7.0711e-01, -1.0000e+00,  7.0711e-01,  1.1925e-08,  4.0000e+00],\n",
       "        [ 7.0711e-01, -1.0000e+00, -7.0711e-01,  1.1925e-08,  4.0000e+00],\n",
       "        [-7.0711e-01,  1.0000e+00, -7.0711e-01, -4.3711e-08,  4.0000e+00],\n",
       "        [-7.0711e-01,  1.0000e+00,  7.0711e-01, -4.3711e-08,  4.0000e+00],\n",
       "        [-7.0711e-01, -1.0000e+00, -7.0711e-01,  1.1925e-08,  4.0000e+00],\n",
       "        [-7.0711e-01, -1.0000e+00,  7.0711e-01,  1.1925e-08,  4.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(4, 2)\n",
    "latt.node_position_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upgrade GraphConv and AutoregressiveModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade from edge-embedding-based to node-embedding-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "class GraphConv(MessagePassing):\n",
    "    \"\"\" Graph Convolution layer \n",
    "        \n",
    "        Args:\n",
    "        in_channels: number of input data channels\n",
    "        out_channels: number of output data channels \n",
    "        edge_features: number of features on the edge\n",
    "        bias: whether to learn an edge depenent bias\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, edge_features: int, bias: bool = True):\n",
    "        super(GraphConv, self).__init__(aggr='add')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_features = edge_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(edge_features, out_channels, in_channels))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(edge_features, out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'in_channels={}, out_channels={}, edge_features={}, bias={}'.format(\n",
    "            self.in_channels, self.out_channels, self.edge_features, self.bias is not None)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        bound = 1 / math.sqrt(self.weight.size(1))\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    def forward(self, input, edge_index, edge_attr):\n",
    "        # input: shape [..., N, in_channels]\n",
    "        # edge_index: shape [2, E]\n",
    "        # edge_attr: shape [E, edge_features]\n",
    "#        print('forward receiving:\\n    input {}\\n    edge_index {}\\n    edge_attr {}'.format(input.size(), edge_index.size(), edge_attr.size()))\n",
    "        return self.propagate(edge_index, input=input, edge_attr=edge_attr)\n",
    "    \n",
    "    def forward_from(self, input, i, edge_index, edge_attr):\n",
    "#        print('--------\\nforward_from receiving:\\n    input {} at {}\\n    edge_index {}\\n    edge_attr {}'.format(input.size(), i, edge_index.size(), edge_attr.size()))\n",
    "        mask = (edge_index[0] == i)\n",
    "#        print('    -> mask found {} active edges to:'.format(mask.sum().item()))\n",
    "#        print('    ', edge_index[1, mask].tolist())\n",
    "        return self(input, edge_index[:, mask], edge_attr[mask, :])\n",
    "    \n",
    "    def message(self, input_j, edge_attr):\n",
    "        # input_j: shape [..., E, in_channels]\n",
    "        # edge_attr_i: [E, edge_features]\n",
    "#        print('message receiving:\\n    input_j {}\\n    edge_attr {}'.format(input_j.size(), edge_attr.size())) \n",
    "        weight = torch.tensordot(edge_attr, self.weight, dims=1)\n",
    "        output = torch.sum(weight * input_j.unsqueeze(-2), -1)\n",
    "        if self.bias is not None:\n",
    "            bias = torch.tensordot(edge_attr, self.bias, dims=1)\n",
    "            output += bias\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class AutoregressiveModel(nn.Module, dist.Distribution):\n",
    "    \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n",
    "        \n",
    "        Args:\n",
    "        lattice: lattice system\n",
    "        channels: a list of channel dimensions from the input layer to the output layer\n",
    "        node_features: node embedding dimension\n",
    "        edge_features: edge embedding dimension\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the additive bias in heap linear layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lattice: LatticeSystem, channels,\n",
    "                 node_features: int = 5, edge_features: int = 4, \n",
    "                 nonlinearity: str = 'ReLU', bias: bool = True):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "        self.lattice = lattice\n",
    "        self.nodes = self.lattice.sites\n",
    "        self.node_index = torch.arange(self.nodes)\n",
    "        self.edge_index = self.lattice.causal_graph()\n",
    "        self.self_loop_index = torch.stack([self.node_index[1:], self.node_index[1:]])\n",
    "        self.node_features = node_features\n",
    "        self.edge_features = edge_features\n",
    "        self.node_attr = self.lattice.node_position_encoding()\n",
    "#        self.node2vec = nn.Embedding(self.nodes, self.node_features,\n",
    "#                                    _weight = self.lattice.node_position_encoding(self.node_features))\n",
    "        self.merger = nn.Bilinear(self.node_features, self.node_features, self.edge_features)\n",
    "        if isinstance(channels, int):\n",
    "            self.channels = [channels, channels]\n",
    "        else:\n",
    "            if channels[0] != channels[-1]:\n",
    "                raise ValueError('In channels {}, the first and last channel dimensions must be equal.'.format(channels))\n",
    "            self.channels = channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in range(1, len(self.channels)):\n",
    "            if l > 1: \n",
    "                self.layers.append(getattr(nn, nonlinearity)())\n",
    "            self.layers.append(GraphConv(self.channels[l - 1], self.channels[l], self.edge_features, bias))\n",
    "        dist.Distribution.__init__(self, event_shape=torch.Size([self.nodes, self.channels[0]]))\n",
    "        self.has_rsample = True\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return '(nodes): {}, (channels): {}\\n(node_features): {}, (edge_features): {}'.format(self.nodes, self.channels, self.node_features, self.edge_features) + super(AutoregressiveModel, self).extra_repr()\n",
    "                \n",
    "    def forward(self, input):\n",
    "        # prepare edge information\n",
    "#        node_attr = self.node2vec(self.node_index) # get node embedding\n",
    "        node_attr = self.node_attr\n",
    "        edge_attr = self.merger(node_attr[self.edge_index[0], :], node_attr[self.edge_index[1], :])\n",
    "        self_loop_attr = self.merger(node_attr[1:], node_attr[1:])\n",
    "        output = input\n",
    "        for l, layer in enumerate(self.layers): # apply layers\n",
    "            if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                if l == 0:\n",
    "                    output = layer(output,\n",
    "                                   self.edge_index,\n",
    "                                   edge_attr)\n",
    "                else:\n",
    "                    output = layer(output,\n",
    "                                   torch.cat([self.edge_index, self.self_loop_index], -1),\n",
    "                                   torch.cat([edge_attr, self_loop_attr], 0))\n",
    "            else: # activation layers\n",
    "                output = layer(output)\n",
    "#            print('forward: l {}'.format(l))\n",
    "#            print(output)\n",
    "        return output # logits\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        logits = self(value) # forward pass to get logits\n",
    "        return torch.sum(value * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "\n",
    "    def sampler(self, logits, dim=-1): # simplified from F.gumbel_softmax\n",
    "        gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "        gumbels += logits\n",
    "        index = gumbels.max(dim, keepdim=True)[1]\n",
    "        return torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "\n",
    "    def _sample(self, batch_size: int, sampler = None):\n",
    "        if sampler is None: # use default sampler\n",
    "            sampler = self.sampler\n",
    "        # create a list of tensors to cache layer-wise outputs\n",
    "        cache = [torch.zeros(batch_size, self.nodes, self.channels[0])]\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                channels = layer.out_channels\n",
    "                cache.append(torch.zeros(batch_size, self.nodes, channels))\n",
    "            else: # activation layers\n",
    "                cache.append(torch.zeros(batch_size, self.nodes, channels))\n",
    "        # prepare edge information\n",
    "#        node_attr = self.node2vec(self.node_index) # get node embedding\n",
    "        node_attr = self.node_attr\n",
    "        edge_attr = self.merger(node_attr[self.edge_index[0], :], node_attr[self.edge_index[1], :])\n",
    "        self_loop_attr = self.merger(node_attr[1:], node_attr[1:])\n",
    "        cache[0][..., 0, :] = sampler(cache[0][..., 0, :]) # always sample node 0 uniformly\n",
    "        for i in range(1, self.nodes):\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                if isinstance(layer, GraphConv): # for graph convolution layers\n",
    "                    if l==0: # first layer\n",
    "                        cache[l + 1] += layer.forward_from(cache[l], i - 1,\n",
    "                                                           self.edge_index,\n",
    "                                                           edge_attr)\n",
    "                    else: # remaining layers\n",
    "                        cache[l + 1] += layer.forward_from(cache[l], i,\n",
    "                                                           torch.cat([self.edge_index, self.self_loop_index], -1),\n",
    "                                                           torch.cat([edge_attr, self_loop_attr], 0))\n",
    "                else: # activation layers\n",
    "                    cache[l + 1][..., i, :] = layer(cache[l][..., i, :])\n",
    "            # the last cache hosts the logit, sample from it \n",
    "            cache[0][..., i, :] = sampler(cache[-1][..., i, :])\n",
    "#            print('forward from {}'.format(i))\n",
    "#            print(cache)\n",
    "        return cache # cache[0] hosts the sample\n",
    "    \n",
    "    def sample(self, batch_size=1):\n",
    "        with torch.no_grad():\n",
    "            cache = self._sample(batch_size)\n",
    "        return cache[0]\n",
    "    \n",
    "    def rsample(self, batch_size=1, tau=None, hard=False):\n",
    "        if tau is None: # if temperature not given\n",
    "            tau = 1/(self.features[-1]-1) # set by the out feature dimension\n",
    "        cache = self._sample(batch_size, lambda x: F.gumbel_softmax(x, tau, hard))\n",
    "        return cache[0]\n",
    "    \n",
    "class HpGCN(nn.Module, dist.TransformedDistribution):\n",
    "    \"\"\" Combination of hierarchical autoregressive and flow-based model for lattice models.\n",
    "    \n",
    "        Args:\n",
    "        energy: a energy model to learn\n",
    "        hidden_features: a list of feature dimensions of hidden layers\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the additive bias in heap linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, energy: EnergyModel, hidden_channels,\n",
    "                 node_features: int = 5, edge_features: int = 4, \n",
    "                 nonlinearity: str = 'ReLU', bias: bool = True):\n",
    "        super(HpGCN, self).__init__()\n",
    "        self.energy = energy\n",
    "        self.group = energy.group\n",
    "        self.lattice = energy.lattice\n",
    "        self.haar = HaarTransform(self.group, self.lattice)\n",
    "        self.onecat = OneHotCategoricalTransform(self.group.order)\n",
    "        channels = [self.group.order] + hidden_channels + [self.group.order]\n",
    "        auto = AutoregressiveModel(self.lattice, channels, node_features, edge_features, nonlinearity, bias)\n",
    "        dist.TransformedDistribution.__init__(self, auto, [self.onecat, self.haar])\n",
    "        self.transform = dist.ComposeTransform(self.transforms) \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]], grad_fn=<SubBackward0>) \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "latt = LatticeSystem(2, 2)\n",
    "ar = AutoregressiveModel(latt, [G.order, 3, G.order])\n",
    "#print(ar.edge_index)\n",
    "with torch.no_grad():\n",
    "    cache = ar._sample(1)\n",
    "dif = ar(cache[0]) - cache[-1]\n",
    "obj = torch.norm(dif)\n",
    "#obj = torch.norm(ar(cache[0]))\n",
    "print('-----\\n',dif,'\\n-----')\n",
    "#with torch.autograd.set_detect_anomaly(True):\n",
    "#    obj.backward()\n",
    "#for p in ar.parameters():\n",
    "#    print(p.size())\n",
    "#    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "latt = LatticeSystem(4, 2)\n",
    "J = 0.2\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "model = HpGCN(EnergyModel(G, latt, H), [8], edge_features=1, nonlinearity='GELU' , bias=True)\n",
    "#model.base_dist.lattice.edge_type.fill_(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -1.3872, free energy: -11.3800\n",
      "loss: -1.2772, free energy: -11.3751\n",
      "loss: -1.8401, free energy: -11.3883\n",
      "loss: -0.6389, free energy: -11.3976\n",
      "loss: -1.4647, free energy: -11.3956\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "free_energy = 0.\n",
    "echo = 100\n",
    "for epoch in range(500):\n",
    "    x = model.sample(batch_size)\n",
    "    log_prob = model.log_prob(x)\n",
    "    energy = model.energy(x)\n",
    "    free = energy + log_prob.detach()\n",
    "    meanfree = free.mean()\n",
    "    loss = torch.sum(log_prob * (free - meanfree))\n",
    "    #l1reg = sum(para.norm(1) for para in model.base_dist.node2vec.parameters())\n",
    "    #tot_loss = loss + 0.001*l1reg\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    free_energy += meanfree.item()\n",
    "    if (epoch+1)%echo == 0:\n",
    "        print('loss: {:.4f}, free energy: {:.4f}'.format(train_loss/echo, free_energy/echo))\n",
    "        train_loss = 0.\n",
    "        free_energy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyModel(\n",
       "  (lattice): LatticeSystem(4x4 grid with tree depth 5)\n",
       "  (energy): EnergyTerms(\n",
       "    (0): TwoBody(tensor([-0.2000,  0.2000]) across (1, 0))\n",
       "    (1): TwoBody(tensor([-0.2000,  0.2000]) across (0, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = 0.2\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "model.energy.update(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4245363175868988\n",
      "1 0.012525171972811222\n",
      "2 0.01250420231372118\n",
      "3 0.012461643666028976\n",
      "4 0.012529365718364716\n",
      "5 0.012554225511848927\n",
      "6 0.0003762754495255649\n",
      "7 0.012512803077697754\n",
      "8 0.012512803077697754\n",
      "9 0.0003762754495255649\n",
      "10 0.012554225511848927\n",
      "11 0.012529365718364716\n",
      "12 0.012461643666028976\n",
      "13 0.01250420231372118\n",
      "14 0.012525171972811222\n",
      "15 0.4245363175868988\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "xs = torch.tensor(list(itertools.product([0,1],repeat=4))).view(-1,2,2)\n",
    "with torch.no_grad():\n",
    "    ps = model.log_prob(xs).exp()\n",
    "for i, p in enumerate(ps):\n",
    "    print(i, p.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0048,  0.0048,  0.0000],\n",
      "        [-0.3605, -0.2768, -0.6904, -0.9490,  1.2043],\n",
      "        [ 1.0732,  0.4588,  0.2340, -0.7635,  0.6221],\n",
      "        [-0.7294,  0.2653,  0.0078, -0.8071,  0.0022],\n",
      "        [-0.0344,  0.4974,  0.0659,  0.3853,  1.5621],\n",
      "        [ 0.7111,  0.6773,  0.1137,  0.2339, -0.2674],\n",
      "        [-0.5738,  0.2787,  0.5201,  0.2133,  0.6567],\n",
      "        [-0.3591,  0.2673, -0.4366,  0.3973, -0.7102],\n",
      "        [ 0.1938, -0.6889, -0.1219,  0.2702,  0.7793],\n",
      "        [-0.3176, -0.3621, -0.4251, -0.2156,  0.3835],\n",
      "        [ 0.1063, -0.0118,  0.3413,  0.5737, -0.0210],\n",
      "        [ 0.0875,  0.8893,  0.7122,  0.4104,  0.3317],\n",
      "        [-0.1232, -0.4791, -0.0235,  0.3320,  0.8548],\n",
      "        [ 0.8407,  0.0113, -0.2151, -0.1885,  0.9369],\n",
      "        [-0.2285,  0.2005, -0.1119,  0.8275,  0.3268],\n",
      "        [ 0.0843,  0.7942,  0.3353,  0.6156,  0.5964]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 1.2061,  0.2099,  0.0906,  0.4469, -0.5653],\n",
      "         [ 0.0121, -0.3850,  0.7348, -0.2888,  0.7334],\n",
      "         [ 0.5102, -0.0902, -0.8977, -0.7095,  0.5757],\n",
      "         [-0.0191,  0.1820,  0.0831,  0.1856,  0.4456],\n",
      "         [-0.4184, -0.6287, -0.3679, -0.3530, -0.1418]],\n",
      "\n",
      "        [[-0.4632,  0.1402, -0.6257,  0.2448, -0.0249],\n",
      "         [-0.8918, -0.0688,  0.1043, -0.9878,  0.1208],\n",
      "         [-0.2997, -0.3113,  0.1323,  0.0210,  0.3289],\n",
      "         [-0.4524, -1.3721, -0.2802,  0.3905,  0.0093],\n",
      "         [-0.5060,  0.1865,  0.0309,  0.4002, -0.0592]],\n",
      "\n",
      "        [[ 0.4989,  0.1658, -0.0825,  0.2013, -0.0797],\n",
      "         [-0.1280,  0.3068, -0.1089,  0.1380, -0.0855],\n",
      "         [ 0.1217, -0.3656, -0.2054, -0.0982, -0.0675],\n",
      "         [-0.2011,  0.2206,  0.0969, -0.2486,  0.6991],\n",
      "         [ 0.0941,  0.1889, -0.1113, -0.1578,  0.3072]],\n",
      "\n",
      "        [[ 0.8020, -0.0378,  0.2894, -0.0606, -0.0522],\n",
      "         [ 0.7541, -0.2123, -0.0629,  0.7183, -0.2237],\n",
      "         [ 0.2023, -0.2182,  0.0255,  0.5737, -0.2375],\n",
      "         [ 0.0439, -0.1912, -0.2451, -0.1352, -0.3756],\n",
      "         [-0.0556,  0.0483, -0.0847, -0.7005,  0.1773]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2095, -0.2873, -0.0442,  0.6605], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-5.1196e-02,  8.2698e-01],\n",
      "         [-6.1980e-02,  1.2679e+00],\n",
      "         [-1.7544e-04,  2.9772e-04],\n",
      "         [-6.3651e-03, -7.0907e-05]],\n",
      "\n",
      "        [[ 3.9314e-01, -1.0896e+00],\n",
      "         [ 2.8353e-01, -9.5395e-01],\n",
      "         [-5.9559e-06, -1.0888e-04],\n",
      "         [ 1.6716e-05,  7.2588e-05]],\n",
      "\n",
      "        [[-4.9685e-02,  1.9166e-01],\n",
      "         [-1.9563e-01,  4.6305e-01],\n",
      "         [ 2.3072e-04,  8.1974e-04],\n",
      "         [-1.4656e-05, -5.9434e-05]],\n",
      "\n",
      "        [[-1.0435e+00,  8.3378e-01],\n",
      "         [ 4.2479e-02,  1.1564e+00],\n",
      "         [ 2.6115e-05, -5.4400e-04],\n",
      "         [-2.0876e-01,  1.2799e-04]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.1777e-01,  9.9195e-02,  1.2248e-04,  5.3426e-02],\n",
      "        [-1.9038e-01, -3.7013e-01,  2.7397e-04,  5.7863e-06],\n",
      "        [ 1.4865e-01,  1.2057e-01,  2.2219e-04, -3.7330e-05],\n",
      "        [-2.8875e-01, -1.1182e-01, -4.2214e-02, -5.1658e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-6.8314e-01,  1.8023e-01, -7.9588e-05,  4.3502e-05],\n",
      "         [ 6.8095e-01, -2.1046e-02,  1.6576e-05, -2.2728e-04]],\n",
      "\n",
      "        [[-6.8454e-01,  3.6054e-01,  7.3680e-05, -6.7001e-05],\n",
      "         [ 6.8327e-01, -1.3397e-01,  1.0751e-05,  5.1797e-05]],\n",
      "\n",
      "        [[ 6.7519e-04,  8.0814e-02,  4.8519e-04, -1.0036e-05],\n",
      "         [-1.3487e-04, -8.0311e-02,  1.3049e-04, -1.5254e-04]],\n",
      "\n",
      "        [[ 3.9017e-01,  1.5680e-01, -1.3201e-04,  9.9724e-05],\n",
      "         [-3.9137e-01,  8.3655e-01, -1.3956e-04,  1.0835e-04]]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2731, -0.5001],\n",
      "        [ 0.1248,  0.3465],\n",
      "        [-0.1005,  0.0219],\n",
      "        [ 0.3608, -0.4681]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for para in model.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "def export(filename, obj):\n",
    "    with open('./data/' + filename + '.json', 'w') as outfile:\n",
    "        outfile.write(jsonpickle.encode(obj))\n",
    "nodevec = list(para.tolist() for para in model.base_dist.node2vec.parameters())\n",
    "export('nodevec', nodevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
