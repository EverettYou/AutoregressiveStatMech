{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierachical Autoregressive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"main.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heap Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange the latent features in a binary heap tree. Each node of the heap tree hosts a feature vector. The `HeapLinear` layer applies linear transformation that maps each node to its causal dependants on the heap tree. A $k$-heap linear transformation is defined as\n",
    "$$y_{2^k m+q}^{j}=\\sum_{i}x_{m}^{i}W_{k}^{ij}+b_{k}^{j}, \\forall q=0,\\cdots,2^k-1,$$\n",
    "where:\n",
    "* $x$ - input features\n",
    "* $y$ - output features\n",
    "* $W$ - weight matrix (depends on $k$)\n",
    "* $b$ - bias vector (depends on $k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node 0 is some what special that it should be thought as $m=\\frac{1}{2}$ in terms of calculating the dependant\n",
    "$$y_{\\lfloor 2^{k-1}\\rfloor+q}^{j}=\\sum_{i}x_{0}^{i}W_{k}^{ij}+b_{k}^{j},\\forall q=0,\\cdots,\\lceil 2^{k-1}\\rceil-1$$\n",
    "In this way, under 0-heap: $0\\to\\{0\\}$, under 1-heap: $0\\to\\{1\\}$, under 2-heap: $0\\to\\{2,3\\}$ and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given the input $x$, the output $y$ will be calculated by adding up all possible $k$-heap linear transformations from $x$ (including the $*$-heap). The following code defines the `HeapLinear` layer with print outs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class HeapLinear(nn.Module):\n",
    "    \"\"\"Applies a heap linear transformation to the incoming data (assuming binary heap)\n",
    "    \n",
    "    Args:\n",
    "        nodes: number nodes in the heap tree (better be 2^n-1) \n",
    "        in_features: size of input features at each heap node\n",
    "        out_features: size of output features at each heap node\n",
    "        bias: whether to learn an additive bias\n",
    "        minheap: minimum heap step to start, must be non-negative int (default = 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes: int, in_features: int, out_features: int, bias: bool = True, minheap: int = 0):\n",
    "        super(HeapLinear, self).__init__()\n",
    "        self.nodes = nodes\n",
    "        self.depth = (nodes-1).bit_length()+1 # fast log2 ceiling\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.minheap = minheap\n",
    "        self.linears = nn.ModuleDict()\n",
    "        for k in range(minheap, self.depth):\n",
    "            self.linears[str(k)] = nn.Linear(in_features, out_features, bias)\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        output = torch.zeros(input.size()[:-1]+(self.out_features,))\n",
    "        for l in range(self.depth):\n",
    "            in_node0 = math.floor(2**(l-1))\n",
    "            in_node1 = 2**l\n",
    "            block_input = input[..., in_node0:in_node1, :]\n",
    "            for k, linear in self.linears.items():\n",
    "                k = int(k)\n",
    "                if in_node0 == 0:\n",
    "                    heap_factor = math.ceil(2**(k-1))\n",
    "                    out_node0 = math.floor(2**(k-1))\n",
    "                    out_node1 = out_node0 + heap_factor\n",
    "                else:\n",
    "                    heap_factor = 2**k\n",
    "                    out_node0 = in_node0 * heap_factor\n",
    "                    out_node1 = in_node1 * heap_factor\n",
    "                if out_node1 <= self.nodes:\n",
    "                    block_output = linear(block_input).repeat_interleave(heap_factor, dim=-2)\n",
    "                    output[..., out_node0:out_node1, :] += block_output\n",
    "                    print('{}-heap:'.format(k),list(range(in_node0,in_node1)),'->',list(range(out_node0,out_node1)))\n",
    "        return output     \n",
    "    \n",
    "    def forward_from(self, input: torch.Tensor, in_node: int):\n",
    "        output = torch.zeros(input.size()[:-2]+(self.nodes, self.out_features,))\n",
    "        in_node_dim = input.size(-2)\n",
    "        if in_node_dim == self.nodes:\n",
    "            input = input[..., [in_node], :]\n",
    "        elif in_node_dim != 1:\n",
    "            raise ValueError('The node dimension must be either {} or 1, get {}.'.format(self.nodes, in_node_dim))\n",
    "        for k, linear in self.linears.items():\n",
    "            k = int(k)\n",
    "            if in_node == 0:\n",
    "                heap_factor = math.ceil(2**(k-1))\n",
    "                out_node0 = math.floor(2**(k-1))\n",
    "                out_node1 = out_node0 + heap_factor\n",
    "            else:\n",
    "                heap_factor = 2**k\n",
    "                out_node0 = in_node * heap_factor\n",
    "                out_node1 = (in_node + 1) * heap_factor\n",
    "            if out_node1 <= self.nodes:\n",
    "                output[..., out_node0:out_node1, :] += linear(input)\n",
    "                print('{}-heap:'.format(k),[in_node],'->',list(range(out_node0,out_node1)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl = HeapLinear(16, 2, 3, minheap=1, bias=True)\n",
    "x = torch.randn(16, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass maps $x$ to $y$ through all possible heaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-heap: [0] -> [1]\n",
      "2-heap: [0] -> [2, 3]\n",
      "3-heap: [0] -> [4, 5, 6, 7]\n",
      "4-heap: [0] -> [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "1-heap: [1] -> [2, 3]\n",
      "2-heap: [1] -> [4, 5, 6, 7]\n",
      "3-heap: [1] -> [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "1-heap: [2, 3] -> [4, 5, 6, 7]\n",
      "2-heap: [2, 3] -> [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "1-heap: [4, 5, 6, 7] -> [8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "ya = hl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/tree.png\" alt=\"tree\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward_from` can initiate the map from a specific node to all its dependants. This will be used in sampling. If the output from every node is summed up, the result should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-heap: [0] -> [1]\n",
      "2-heap: [0] -> [2, 3]\n",
      "3-heap: [0] -> [4, 5, 6, 7]\n",
      "4-heap: [0] -> [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "1-heap: [1] -> [2, 3]\n",
      "2-heap: [1] -> [4, 5, 6, 7]\n",
      "3-heap: [1] -> [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "1-heap: [2] -> [4, 5]\n",
      "2-heap: [2] -> [8, 9, 10, 11]\n",
      "1-heap: [3] -> [6, 7]\n",
      "2-heap: [3] -> [12, 13, 14, 15]\n",
      "1-heap: [4] -> [8, 9]\n",
      "1-heap: [5] -> [10, 11]\n",
      "1-heap: [6] -> [12, 13]\n",
      "1-heap: [7] -> [14, 15]\n"
     ]
    }
   ],
   "source": [
    "yb = sum(hl.forward_from(x, i) for i in range(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `forward` and `forward_from` results are indeed consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya - yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoregressiveModel` uses heap linear layer to realize the hierachical causal structure. It provides the functionality to generate samples and calculating log probabilities. Each sample should be a 2D tensor of the shape $(N_\\text{unit}, d_\\text{states})$, where $N_\\text{unit}$ is the number of units (nodes), and $d_\\text{states}$ is the number of states for each unit (number of physical features). Samples may come in batches with arbitary batch shape. The hierachical autoregressive model models the probability of a sample $x$ as\n",
    "$$p(x)=p(x_0)p(x_1|x_0)p(x_2|x_0,x_1)p(x_3|x_0,x_1)p(x_4|x_0,x_1,x_2)p(x_5|x_0,x_1,x_2)p(x_6|x_0,x_1,x_3)p(x_7|x_0,x_1,x_3)\\cdots$$\n",
    "The conditional distributions are modeled by neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/prob_tree.png\" alt=\"prob_tree\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible issue:\n",
    "* The connection is too sparse, the causal cone is too narrow. $x_5,x_6$ hard to establish corelation.\n",
    "* Why the same heap level need to share weight? Can we pass different message to different child? Currently, all nodes on the same leaf have identical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"auto.py\"\n",
    "class AutoregressiveModel(nn.Module, dist.Distribution):\n",
    "    \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n",
    "        \n",
    "        Args:\n",
    "        units: number of units in the model\n",
    "        features: a list of feature dimensions from the input layer to the output layer\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the additive bias in heap linear layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units: int, features, nonlinearity: str = 'ReLU', bias: bool = True):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "        self.units = units\n",
    "        self.features = features\n",
    "        if features[0] != features[-1]:\n",
    "            raise ValueError('In features {}, the first and last feature dimensions must be equal.'.format(features))\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in range(len(features)-1):\n",
    "            if l == 0: # first heap linear layer must have minheap=1\n",
    "                self.layers.append(HeapLinear(units, features[0], features[1], bias, minheap = 1))\n",
    "            else: # remaining heap linear layers have minheap=0 (by default)\n",
    "                self.layers.append(getattr(nn, nonlinearity)())\n",
    "                self.layers.append(HeapLinear(units, features[l], features[l+1], bias))\n",
    "        dist.Distribution.__init__(self, event_shape=torch.Size([units, features[0]]))\n",
    "        self.has_rsample = True\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return '(units): {}\\n(features): {}'.format(self.units, self.features) + super(AutoregressiveModel, self).extra_repr()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        logits = input # logits as a workspace, initialized to input\n",
    "        for layer in self.layers: # apply layers\n",
    "            logits = layer(logits)\n",
    "        return logits # logits output\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        logits = self(value) # forward pass to get logits\n",
    "        return torch.sum(value * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "        \n",
    "    def _sample(self, batch_size: int, sampler = None):\n",
    "        if sampler is None: # use default sampler\n",
    "            sampler = self.sampler\n",
    "        # create a list of tensors to cache layer-wise outputs\n",
    "        cache = [torch.zeros(batch_size, self.units, feature) for feature in self.features]\n",
    "        # autoregressive batch sampling\n",
    "        for i in range(self.units):\n",
    "            for l in range(len(self.features)-1):\n",
    "                if l==0: # first linear layer\n",
    "                    if i > 0:\n",
    "                        cache[1] += self.layers[0].forward_from(cache[0], i - 1) # heap linear\n",
    "                else: # remaining layers\n",
    "                    activation = self.layers[2*l-1](cache[l][..., [i], :]) # element-wise\n",
    "                    cache[l + 1] += self.layers[2*l].forward_from(activation, i) # heap linear\n",
    "            # the last record hosts logits \n",
    "            cache[0][..., i, :] = sampler(cache[-1][..., i, :])\n",
    "        return cache # cache[0] hosts the sample\n",
    "        \n",
    "    def sampler(self, logits, dim=-1): # simplified from F.gumbel_softmax\n",
    "        gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "        gumbels += logits\n",
    "        index = gumbels.max(dim, keepdim=True)[1]\n",
    "        return torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "    \n",
    "    def sample(self, batch_size=1):\n",
    "        with torch.no_grad():\n",
    "            cache = self._sample(batch_size)\n",
    "        return cache[0]\n",
    "    \n",
    "    def rsample(self, batch_size=1, tau=None, hard=False):\n",
    "        if tau is None: # if temperature not given\n",
    "            tau = 1/(self.features[-1]-1) # set by the out feature dimension\n",
    "        cache = self._sample(batch_size, lambda x: F.gumbel_softmax(x, tau, hard))\n",
    "        return cache[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains the following components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoregressiveModel(\n",
       "  (units): 8\n",
       "  (features): [2, 3, 2]\n",
       "  (layers): ModuleList(\n",
       "    (0): HeapLinear(\n",
       "      (linears): ModuleDict(\n",
       "        (1): Linear(in_features=2, out_features=3, bias=True)\n",
       "        (2): Linear(in_features=2, out_features=3, bias=True)\n",
       "        (3): Linear(in_features=2, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): HeapLinear(\n",
       "      (linears): ModuleDict(\n",
       "        (0): Linear(in_features=3, out_features=2, bias=True)\n",
       "        (1): Linear(in_features=3, out_features=2, bias=True)\n",
       "        (2): Linear(in_features=3, out_features=2, bias=True)\n",
       "        (3): Linear(in_features=3, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = AutoregressiveModel(8, [2,3,2], bias=True)\n",
    "ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the forward pass recovers the logits generated through the autoregressive sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache = ar._sample(1)\n",
    "ar(cache[0]) - cache[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot and Categorical Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample draw from the autoregressive model are one-hot embeddings. Convert between one-hot and categorical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotCategoricalTransform(dist.Transform):\n",
    "    \"\"\"Convert between one-hot and categorical representations.\n",
    "    \n",
    "    Args:\n",
    "    num_classes: number of classes.\"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(OneHotCategoricalTransform, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.bijective = True\n",
    "    \n",
    "    def _call(self, x):\n",
    "        # one-hot to categorical\n",
    "        return x.max(dim=-1)[1]\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        # categorical to one-hot\n",
    "        return F.one_hot(y, self.num_classes).to(dtype=torch.float)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = AutoregressiveModel(4, [2,3,2], bias=False)\n",
    "x = ar.sample(2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0],\n",
       "        [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc = OneHotCategoricalTransform(2)\n",
    "y = oc(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc.inv(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be pack into a Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Group` represents a group specified by the multiplication table. Group elements will be labeled by integers (ranging from 0 to the order of the group). The element 0 is always treated as the identity element of the group. `Group` provides methods to perform element-wise group multiplication for Torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    \"\"\"Represent a group, providing multiplication and inverse operation.\n",
    "    \n",
    "    Args:\n",
    "    mul_table: multiplication table as a tensor, e.g. Z2 group: tensor([[0,1],[1,0]])\n",
    "    \"\"\"\n",
    "    def __init__(self, mul_table: torch.Tensor):\n",
    "        super(Group, self).__init__()\n",
    "        self.mul_table = mul_table\n",
    "        self.order = mul_table.size(0) # number of group elements\n",
    "        gs, ginvs = torch.nonzero(self.mul_table == 0, as_tuple=True)\n",
    "        self.inv_table = torch.gather(ginvs, 0, gs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(range(self.order))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Group({} elements)'.format(self.order)\n",
    "    \n",
    "    def inv(self, input: torch.Tensor):\n",
    "        return torch.gather(self.inv_table.expand(input.size()[:-1]+(-1,)), -1, input)\n",
    "    \n",
    "    def mul(self, input1: torch.Tensor, input2: torch.Tensor):\n",
    "        output = input1 * self.order + input2\n",
    "        return torch.gather(self.mul_table.flatten().expand(output.size()[:-1]+(-1,)), -1, output)\n",
    "    \n",
    "    def prod(self, input, dim: int, keepdim: bool = False):\n",
    "        input_size = input.size()\n",
    "        flat_mul_table = self.mul_table.flatten().expand(input_size[:dim]+input_size[dim+1:-1]+(-1,))\n",
    "        output = input.select(dim, 0)\n",
    "        for i in range(1, input.size(dim)):\n",
    "            output = output * self.order + input.select(dim, i)\n",
    "            output = torch.gather(flat_mul_table, -1, output)\n",
    "        if keepdim:\n",
    "            output = output.unsqueeze(dim)\n",
    "        return output\n",
    "    \n",
    "    def val(self, input, val_table = None):\n",
    "        if val_table is None:\n",
    "            val_table = torch.zeros(self.order)\n",
    "            val_table[0] = 1.\n",
    "        elif len(val_table) != self.order:\n",
    "            raise ValueError('Group function value table must be of the same size as the group order, expect {} got {}.'.format(self.order, len(val_table)))\n",
    "        return torch.gather(val_table.expand(input.size()[:-1]+(-1,)), -1, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a $S_3$ group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Group(torch.tensor([[0,1,2,3,4,5],[1,0,3,2,5,4],[2,4,0,5,1,3],[3,5,1,4,0,2],[4,2,5,0,3,1],[5,3,4,1,2,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying two tensors element-wise following the group multiplication rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5],\n",
       "        [1, 2, 5]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0,1,2],[3,4,5]])\n",
    "b = torch.tensor([[5,4,3],[2,1,0]])\n",
    "G.mul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product of each row of a tensor in the given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 3])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.prod(a, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group inversion of all elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [4, 3, 5]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.inv(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a group function given by a value table `val_table` (default function: group delta function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.val(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "        [-0.5000, -0.5000,  0.0000]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.val(a, val_table=torch.tensor([1.,0.,0.,-0.5,-0.5,0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lattice System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LatticeSystem(G, L, d)` is only a container to host lattice information which could be passed together to other classes. It defines the group $G$ on each site, the size $L$ and the dimension $d$ of the lattice. Such that the lattice grid is of the shape $L\\times\\cdots\\times L$ ($d$ times), and the total number units (sites) is $L^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatticeSystem(object):\n",
    "    \"\"\" a container to host lattice information\n",
    "        \n",
    "        Args:\n",
    "        group: a group that defines multiplication among elements\n",
    "        size: length of the lattcie along one dimension (should be a power of 2)\n",
    "        dimension: dimension of the lattice\n",
    "    \"\"\"\n",
    "    def __init__(self, group: Group, size: int, dimension: int):\n",
    "        super(LatticeSystem, self).__init__()\n",
    "        self.group = group\n",
    "        self.size = size\n",
    "        self.dimension = dimension\n",
    "        self.shape = [size]*dimension\n",
    "        self.units = size**dimension\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'LatticeSystem({} on {} grid)'.format(self.group, 'x'.join(str(L) for L in self.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 4], 16)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(G, 4, 2)\n",
    "latt.shape, latt.units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haar Wavelet Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HaarWavelet(latt)` returns the Haar wavelet basis on lattice. The lattice size $L$ should be $2^n$ such that the bipartition can be performed to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HaarWavelet(latt: LatticeSystem):\n",
    "    wav = torch.zeros(torch.Size([latt.units] + latt.shape), dtype=torch.int)\n",
    "    def partition(rng: torch.Tensor, dim: int, ind: int):\n",
    "        if rng[dim].sum()%2 == 0:\n",
    "            mid = rng[dim].sum()//2\n",
    "            rng1 = rng.clone()\n",
    "            rng1[dim, 1] = mid\n",
    "            rng2 = rng.clone()\n",
    "            rng2[dim, 0] = mid\n",
    "            w = wav[ind]\n",
    "            for k in range(rng1.size(0)):\n",
    "                w = w.narrow(k, rng1[k,0], rng1[k,1]-rng1[k,0])\n",
    "            w.fill_(1)\n",
    "            partition(rng1, (dim + 1)%latt.dimension, 2*ind)\n",
    "            partition(rng2, (dim + 1)%latt.dimension, 2*ind + 1)\n",
    "    partition(torch.tensor([[0, latt.size]]*latt.dimension), 0, 1)\n",
    "    wav[0] = 1\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Haar wavelet of size 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HaarWavelet(LatticeSystem(G, 8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D Haar wavelet of size $2\\times 2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1, 0],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[0, 0],\n",
       "         [1, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HaarWavelet(LatticeSystem(G, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Slice by Coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`coordinate_select` is a helper function, which selects element form input by coordinate index. The coordinate can be any iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_select(input: torch.Tensor, coordinate, dims = None):\n",
    "    output = input\n",
    "    if dims is None:\n",
    "        dims = range(len(coordinate))\n",
    "    for dim, i in zip(dims, coordinate):\n",
    "        output = output.narrow(dim, i, 1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(8).reshape(2,2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3]]])"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate_select(x, (0,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 5]]])"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate_select(x, (1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2]],\n",
       "\n",
       "        [[6]]])"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate_select(x, (1,0), dims=(-2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haar Transformation Bijector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haar wavelet transformation realized as a bijective transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "class HaarTransform(dist.Transform):\n",
    "    \"\"\"Haar wavelet transformation (bijective)\n",
    "    transformation takes real space configurations x to wavelet space encoding y\n",
    "    \n",
    "    Args:\n",
    "    lattice: a lattice system containing information of the group and lattice shape\n",
    "    \"\"\"\n",
    "    def __init__(self, lattice: LatticeSystem):\n",
    "        super(HaarTransform, self).__init__()\n",
    "        self.lattice = lattice\n",
    "        self.bijective = True\n",
    "        self.make_wavelet()\n",
    "        self.make_plan()\n",
    "        \n",
    "    # construct Haar wavelet basis\n",
    "    def make_wavelet(self):\n",
    "        self.wavelet = torch.zeros(torch.Size([self.lattice.units]+self.lattice.shape), dtype=torch.int)\n",
    "        def partition(rng: torch.Tensor, dim: int, ind: int):\n",
    "            if rng[dim].sum()%2 == 0:\n",
    "                mid = rng[dim].sum()//2\n",
    "                rng1 = rng.clone()\n",
    "                rng1[dim, 1] = mid\n",
    "                rng2 = rng.clone()\n",
    "                rng2[dim, 0] = mid\n",
    "                wave = self.wavelet[ind]\n",
    "                for k in range(rng1.size(0)):\n",
    "                    wave = wave.narrow(k, rng1[k,0], rng1[k,1]-rng1[k,0])\n",
    "                wave.fill_(1)\n",
    "                partition(rng1, (dim + 1)%self.lattice.dimension, 2*ind)\n",
    "                partition(rng2, (dim + 1)%self.lattice.dimension, 2*ind + 1)\n",
    "        partition(torch.tensor([[0, self.lattice.size]]*self.lattice.dimension), 0, 1)\n",
    "        self.wavelet[0] = 1\n",
    "    \n",
    "    # construct solution plan for Haar decomposition\n",
    "    def make_plan(self):\n",
    "        levmap = self.wavelet.sum(0)\n",
    "        self.plan = {i:[] for i in range(self.lattice.units)}\n",
    "        for spot in zip(*torch.nonzero(self.wavelet, as_tuple = True)):\n",
    "            self.plan[spot[0].item()].append(tuple(x.item() for x in spot[1:]))\n",
    "        spot2lev = {spot: coordinate_select(levmap, spot).item() for spot in \n",
    "                    itertools.product(*[range(d) for d in self.lattice.shape])}\n",
    "        self.plan = {i: sorted(spots, key=lambda spot: spot2lev[spot]) for i, spots in self.plan.items()}\n",
    "        \n",
    "    def _call(self, x):\n",
    "        wave = self.wavelet.view(torch.Size([1]*(x.dim()-1))+self.wavelet.size())\n",
    "        x = x.view(x.size() + torch.Size([1]*self.lattice.dimension))\n",
    "        return self.lattice.group.prod(x * wave, -(self.lattice.dimension+1))\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        y = y.clone() # to avoid modifying the original input\n",
    "        x = torch.zeros(y.size()[:-self.lattice.dimension]+(self.lattice.units,), dtype=torch.long)\n",
    "        dims = tuple(range(-self.lattice.dimension,0))\n",
    "        for i, spots in self.plan.items():\n",
    "            sol = coordinate_select(y, spots[0], dims)\n",
    "            x[...,i] = sol.squeeze()\n",
    "            invsol = self.lattice.group.inv(sol)\n",
    "            for spot in spots[1:]:\n",
    "                y_spot = coordinate_select(y, spot, dims)\n",
    "                y_spot.copy_(self.lattice.group.mul(invsol, y_spot))\n",
    "        return x\n",
    "    \n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration. Create a autoregressive model and sample some configuration of Haar encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0],\n",
       "        [0, 1, 0, 1],\n",
       "        [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = AutoregressiveModel(4, [2,3,2], bias=False)\n",
    "oc = OneHotCategoricalTransform(2)\n",
    "x = oc(ar.sample(3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Haar transform and decode to spin configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [1, 0]],\n",
       "\n",
       "        [[1, 0],\n",
       "         [1, 0]]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht = HaarTransform(LatticeSystem(Group(torch.tensor([[0,1],[1,0]])), 2, 2))\n",
    "y = ht(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode spin configuration back to Haar encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0],\n",
       "        [0, 1, 0, 1],\n",
       "        [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.inv(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-trivial examples of non-Abelian groups on 3D lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5, 0, 5, 3, 2, 1, 0, 0, 1, 0, 2, 4, 5, 0, 2, 2, 3, 5, 5, 3, 4, 2, 3, 2,\n",
       "          0, 4, 3, 4, 5, 3, 3, 1, 5, 0, 5, 4, 3, 1, 1, 2, 4, 1, 4, 1, 3, 4, 2, 3,\n",
       "          3, 1, 0, 2, 1, 3, 3, 4, 5, 3, 1, 0, 1, 4, 2, 3]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Group(torch.tensor([[0,1,2,3,4,5],[1,0,3,2,5,4],[2,4,0,5,1,3],[3,5,1,4,0,2],[4,2,5,0,3,1],[5,3,4,1,2,0]]))\n",
    "ht = HaarTransform(LatticeSystem(G, 4, 3))\n",
    "oc = OneHotCategoricalTransform(G.order)\n",
    "ar = AutoregressiveModel(ht.lattice.units, [G.order,3,G.order], bias=False)\n",
    "x = oc(ar.sample(1))\n",
    "x, x - ht.inv(ht(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need an energy model to describe the Statistical Mechanics system. `EnergyModel` provides the function to evalutate the energy of a configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyModel(nn.Module):\n",
    "    \"\"\" Energy mdoel that describes the physical system. Provides function to evaluate energy.\n",
    "    \n",
    "        Args:\n",
    "        lattice: a lattice system containing information of the group and lattice shape\n",
    "        hamiltonian: lattice Hamiltonian in terms of energy terms\n",
    "    \"\"\"\n",
    "    def __init__(self, lattice: LatticeSystem, energy: EnergyTerms):\n",
    "        super(EnergyModel, self).__init__()\n",
    "        self.lattice = lattice\n",
    "        self.energy = energy.on(lattice)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return '(lattice): {}'.format(self.lattice) + super(EnergyModel, self).extra_repr()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.energy(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 2D Ising model on a square lattice\n",
    "$$H= -J \\sum_{i}(\\sigma_i\\sigma_{i+\\hat{x}} + \\sigma_i\\sigma_{i+\\hat{y}}).$$\n",
    "The Hamiltonian can be typed in as (see the following subsection for explaination of the notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 0.7\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.], dtype=float), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.], dtype=float), (0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy model is defined by the lattice system and the Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyModel(\n",
       "  (lattice): LatticeSystem(Group(2 elements) on 4x4 grid)\n",
       "  (hamiltonian): EnergyTerms(\n",
       "    (0): TwoBody(tensor([-0.7000,  0.7000], dtype=torch.float64) across (1, 0))\n",
       "    (1): TwoBody(tensor([-0.7000,  0.7000], dtype=torch.float64) across (0, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt = LatticeSystem(Group(torch.tensor([[0,1],[1,0]])), 4, 2)\n",
    "energy = EnergyModel(latt, H)\n",
    "energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some spin configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 0],\n",
       "         [1, 0, 1, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         [0, 1, 1, 1]],\n",
       "\n",
       "        [[1, 0, 1, 1],\n",
       "         [0, 1, 1, 0],\n",
       "         [1, 0, 0, 1],\n",
       "         [1, 1, 1, 0]],\n",
       "\n",
       "        [[0, 0, 1, 0],\n",
       "         [1, 0, 1, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = AutoregressiveModel(latt.units, [latt.group.order,3,latt.group.order], bias=False)\n",
    "oc = OneHotCategoricalTransform(latt.group.order)\n",
    "ht = HaarTransform(latt)\n",
    "x = ht(oc(ar.sample(3)))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy model can be used to evalutate the energy of these spin configuraitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8000, 5.6000, 2.8000], dtype=torch.float64)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hamiltonian Scripting System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(See [phys.py](./phys.py) for code details).\n",
    "\n",
    "In order to facilitate the intuitive formulation of Hamiltonian, we have introduced a scripting system. Physical Hamiltonians are always sum of local energy terms. In this system, each energy term is a subclass of `nn.Module` and each Hamiltonian is a subclass of `nn.ModuleList` (which contains the collection of energy terms). In this way, the evaluation of the total energy of the Hamiltonian can be passed down to each energy terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced two kinds of energy terms\n",
    "* `OnSite`: on-site energy term $E_1(g_i)$,\n",
    "* `TwoBody`: two-body interaction term $E_2(g_i,g_j)$.\n",
    "\n",
    "More complicated interaction terms can be introduced under this framework if necessary. These energy terms are group functions: $E_1:G\\to\\mathbb{R}$, $E_2:G\\times G\\to\\mathbb{R}$. These group functions can be specified by a value table, which enumerated the value that each group element maps to. For example, for the $\\mathbb{Z}_2=\\{0,1\\}$ group ($0$-identity, $1$-generator), if we want to specify $E_1(g_\\sigma)=\\sigma$, i.e.\n",
    "$$E_1(0)=+1, E_1(1)=-1,$$\n",
    "the value talbe is $[+1,-1]$. Such a term can be created as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OnSite(tensor([ 1., -1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnSite(torch.tensor([1.,-1.],dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-body term, we assume that it always take the form of\n",
    "$$E_2(g_i,g_j)=E_2(g_i^{-1}g_j),$$\n",
    "such that we will only need to a single-variable group function, unsing the same value table representation. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoBody(tensor([ 1., -1.], dtype=torch.float64) across (1, 0))"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwoBody(torch.tensor([1.,-1.],dtype=float), (1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-body term also carries a second argument to specify the relative direction from site-$i$ to site-$j$. If the value table is not specified, the default group function will be taken to be the delta function (like Potts model), which maps the identity element to 1 and the others to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this system, we can add, subtract, scalar multiply and negate the energy terms. Energy terms adding together will be represented as a collection of terms in a list (`nn.ModuleList`), which correspond to a Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyTerms(\n",
       "  (0): TwoBody(5.2)\n",
       "  (1): TwoBody(5.2)\n",
       "  (2): OnSite(-2.8)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.8 * OnSite() + 5.2 * (TwoBody(shifts=(1,0)) + TwoBody(shifts=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Hamiltonian `H` needs to be further put on a lattice by calling `H.on(lattice)`. Only after it putting on a lattice, the Hamiltonian has a concrete meaning, and then the energy of the system can be evaluated by the `H.forward(input)` method on a spin configuration as `input`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HolographicPixelFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be a better name, but the idea is that the holographic pixel flow combines components of renormalization group, autoregressive model (pixel-...), and flow-based generative model together to create a probability model for modeling critical statistical mechanics systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolographicPixelFlow(nn.Module, dist.TransformedDistribution):\n",
    "    \"\"\" Combination of hierarchical autoregressive and flow-based model for lattice models.\n",
    "    \n",
    "        Args:\n",
    "        model: a energy model to learn\n",
    "        hidden_features: a list of feature dimensions of hidden layers\n",
    "        nonlinearity: activation function to use \n",
    "        bias: whether to learn the additive bias in heap linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, model: EnergyModel, hidden_features, nonlinearity: str = 'ReLU', bias: bool = True):\n",
    "        super(HolographicPixelFlow, self).__init__()\n",
    "        self.model = model\n",
    "        self.haar = HaarTransform(model.lattice)\n",
    "        n = model.lattice.group.order\n",
    "        self.onecat = OneHotCategoricalTransform(n)\n",
    "        features = [n] + hidden_features + [n]\n",
    "        auto = AutoregressiveModel(model.lattice.units, features, nonlinearity, bias)\n",
    "        dist.TransformedDistribution.__init__(self, auto, [self.onecat, self.haar])\n",
    "        self.transform = dist.ComposeTransform(self.transforms)\n",
    "        \n",
    "    def energy(self, input): # create a shortcut for energy\n",
    "        return self.model.energy(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a holographic pixel flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HolographicPixelFlow(\n",
       "  (model): EnergyModel(\n",
       "    (lattice): LatticeSystem(Group(2 elements) on 4x4 grid)\n",
       "    (energy): EnergyTerms(\n",
       "      (0): TwoBody(tensor([-0.7000,  0.7000]) across (1, 0))\n",
       "      (1): TwoBody(tensor([-0.7000,  0.7000]) across (0, 1))\n",
       "    )\n",
       "  )\n",
       "  (base_dist): AutoregressiveModel(\n",
       "    (units): 16\n",
       "    (features): [2, 3, 2]\n",
       "    (layers): ModuleList(\n",
       "      (0): HeapLinear(\n",
       "        (linears): ModuleDict(\n",
       "          (1): Linear(in_features=2, out_features=3, bias=False)\n",
       "          (2): Linear(in_features=2, out_features=3, bias=False)\n",
       "          (3): Linear(in_features=2, out_features=3, bias=False)\n",
       "          (4): Linear(in_features=2, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): HeapLinear(\n",
       "        (linears): ModuleDict(\n",
       "          (0): Linear(in_features=3, out_features=2, bias=False)\n",
       "          (1): Linear(in_features=3, out_features=2, bias=False)\n",
       "          (2): Linear(in_features=3, out_features=2, bias=False)\n",
       "          (3): Linear(in_features=3, out_features=2, bias=False)\n",
       "          (4): Linear(in_features=3, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run \"main.py\"\n",
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "J = 0.7\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "hpf = HolographicPixelFlow(EnergyModel(LatticeSystem(G, 4, 2), H), [3], bias = False)\n",
    "hpf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw samples from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 1],\n",
       "         [1, 0, 0, 0],\n",
       "         [1, 0, 1, 0],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 1, 0, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [1, 0, 1, 0],\n",
       "         [0, 1, 0, 1]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hpf.sample(2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate log probabilities of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.9535, -10.2614], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpf.log_prob(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate energies of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8000, 8.4000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpf.energy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform samples to the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpf.transform.inv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reverse KL with log-trick**. The goal is to minimize the difference between the model distribution $q_\\theta(x)$ and the target distribution $p(x) \\propto e^{-E(x)}$ by minimizing the reverse KL divergence\n",
    "$$\\begin{split}\\mathcal{L}&=\\mathsf{KL}(q_\\theta||p)\\\\\n",
    "&=\\sum_{x} q_\\theta(x) \\ln \\frac{q_\\theta(x)}{p(x)}\\\\\n",
    "&=\\sum_{x}q_\\theta(x)(E(x)+\\ln q_\\theta(x)). \n",
    "\\end{split}$$\n",
    "The parameter dependence is only in $q_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is given by\n",
    "$$\\begin{split}\\partial_\\theta\\mathcal{L}&= \\partial_\\theta \\sum_{x}q_\\theta(x)(E(x)+\\ln q_\\theta(x))\\\\\n",
    "&= \\sum_{x}[(\\partial_\\theta q_\\theta(x))(E(x)+\\ln q_\\theta(x))+q_\\theta(x)\\partial_\\theta \\ln q_\\theta(x)]\\\\\n",
    "\\end{split}$$\n",
    "The last term can be dropped because \n",
    "$$\\sum_x q_\\theta(x)\\partial_\\theta \\ln q_\\theta(x) = \\sum_x \\partial_\\theta q_\\theta(x)=\\partial_\\theta\\sum_x q_\\theta(x)=\\partial_\\theta 1 = 0,$$\n",
    "the remaining term reads\n",
    "$$\\begin{split}\\partial_\\theta\\mathcal{L}&= \\sum_{x}(\\partial_\\theta q_\\theta(x))(E(x)+\\ln q_\\theta(x))\\\\\n",
    "&= \\sum_{x}(\\partial_\\theta q_\\theta(x))R(x)\\\\\n",
    "&= \\mathbb{E}_{x\\sim q_\\theta}(\\partial_\\theta \\ln q_\\theta(x))R(x)\\\\\n",
    "\\end{split}$$\n",
    "with a reward signal $R(x)=E(x)+\\ln q_\\theta(x)$ in the context of reinforcement learning. The gradient signal $\\partial_\\theta \\ln q_\\theta(x)$ is weighted by $R(x)$, such that when $R(x)$ is large for a configuration $x$, the gradient descent will decrease the log likelihood $\\ln q_\\theta(x)$ for that configuration, hence the optimzation will try to reduce the free energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we should not just drop the last term for finite batches, instead we should introduce a Lagrangian multiplier to counter the gradient signal that is towards the direction of violating the normalization condition. This amounts to subtracting $R(x)$ by a baseline value $b=\\mathbb{E}_{x\\sim q_\\theta} R(x)$, which can be estimated within each batch. The baseline subtraction helps to reduce the variance of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"main.py\"\n",
    "G = Group(torch.tensor([[0,1],[1,0]]))\n",
    "J = 0.1\n",
    "H = - J * (TwoBody(torch.tensor([1.,-1.]), (1,0)) \n",
    "           + TwoBody(torch.tensor([1.,-1.]), (0,1)))\n",
    "hpf = HolographicPixelFlow(EnergyModel(LatticeSystem(G, 2, 2), H), [16, 16], bias = False)\n",
    "optimizer = optim.Adam(hpf.parameters(), lr=0.001)\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0629, free energy: -2.8348\n",
      "loss: -0.0035, free energy: -2.8362\n",
      "loss: 0.0205, free energy: -2.8353\n",
      "loss: -0.0243, free energy: -2.8353\n",
      "loss: 0.0591, free energy: -2.8360\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.\n",
    "free_energy = 0.\n",
    "echo = 100\n",
    "for epoch in range(500):\n",
    "    x = hpf.sample(batch_size)\n",
    "    log_prob = hpf.log_prob(x)\n",
    "    energy = hpf.energy(x)\n",
    "    free = energy + log_prob.detach()\n",
    "    meanfree = free.mean()\n",
    "    loss = torch.sum(log_prob * (free - meanfree))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    free_energy += meanfree.item()\n",
    "    if (epoch+1)%echo == 0:\n",
    "        print('loss: {:.4f}, free energy: {:.4f}'.format(train_loss/echo, free_energy/echo))\n",
    "        train_loss = 0.\n",
    "        free_energy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[0, 0],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[0, 0],\n",
       "         [1, 0]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1, 0],\n",
       "         [0, 1]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hpf.sample(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0473, 0.1099, 0.1099, 0.1168, 0.1168], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpf.log_prob(x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.tensor(list(itertools.product([0,1],repeat=4))).view(-1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.11705994606018066\n",
      "1 0.04812687262892723\n",
      "2 0.07097935676574707\n",
      "3 0.06810590624809265\n",
      "4 0.047985441982746124\n",
      "5 0.04294325038790703\n",
      "6 0.03375312313437462\n",
      "7 0.07086819410324097\n",
      "8 0.07097935676574707\n",
      "9 0.03400873765349388\n",
      "10 0.04303836077451706\n",
      "11 0.04812687262892723\n",
      "12 0.06821898370981216\n",
      "13 0.07086819410324097\n",
      "14 0.047985441982746124\n",
      "15 0.11695203930139542\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ps = hpf.log_prob(xs).exp()\n",
    "for i, p in enumerate(ps):\n",
    "    print(i, p.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpf.transform.inv(xs[[0,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [ 0.1514, -0.2736],\n",
       "         [ 0.0701, -0.3479],\n",
       "         [ 0.0701, -0.3479]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [ 0.1498, -0.2706],\n",
       "         [-0.0920, -0.5900],\n",
       "         [-0.0920, -0.5900]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpf.base_dist(hpf.transform.inv(xs[[0,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
