{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import torch.optim as optim\n",
    "import torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine CPU or GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph object stores the adjacency matrix in a sparse form.\n",
    "- **Depth assignment algorithm**. Let $A$ be the adjacency matrix. $u$ is an one-hot vector encoding the active vertices. Initially, $u$ is set to all one. $d$ is the depth vector, initially assigned to all zero.\n",
    "  - $u'= \\text{bool}(A u > 0)$ gives the target vertices under adjacency map,\n",
    "  - if $\\Vert u'\\Vert_1=\\Vert u\\Vert_1$ stop, otherwise $d=d+u'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "    \"\"\" Host graph information and enables graph expansion\n",
    "        \n",
    "        Args:\n",
    "        dims: (target_dim, source_dim) number of target/source variables\n",
    "        indices: index list of adjacency matrix [2, edge_num]\n",
    "        edge_types: edge type list of adjacency matrix [edge_num]\n",
    "        source_depths (optional): depth assignments of source variables\n",
    "    \"\"\"\n",
    "    def __init__(self, dims:int or tuple, indices, edge_types, source_depths=None):\n",
    "        if isinstance(dims, int):\n",
    "            self.dims = (dims, dims)\n",
    "        elif isinstance(dims, tuple):\n",
    "            self.dims = dims\n",
    "        self.indices = indices\n",
    "        self.edge_types = edge_types\n",
    "        self.max_edge_type = edge_types.max().item()\n",
    "        if source_depths is None:\n",
    "            self.source_depths = self.get_depth_assignment()\n",
    "        else:\n",
    "            self.source_depths = source_depths\n",
    "        self.edge_depths = self.source_depths[self.indices[1,:]]\n",
    "        self.max_depth = self.source_depths.max().item()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Graph({}, {} edges of {} types)'.format('x'.join(str(v) for v in self.dims), self.edge_types.shape[0], self.max_edge_type)\n",
    "    \n",
    "    def adjacency_matrix(self):\n",
    "        return torch.sparse_coo_tensor(self.indices, self.edge_types, self.dims)\n",
    "    \n",
    "    def get_depth_assignment(self):\n",
    "        assert self.dims[0] == self.dims[1], 'get_depths can only be called with square adjacency matrix.'\n",
    "        dvec = torch.zeros(self.dims[0], dtype=torch.long)\n",
    "        uvec = torch.ones(self.dims[0], dtype=torch.long)\n",
    "        adjmat = self.adjacency_matrix()\n",
    "        while True:\n",
    "            uvec_new = (adjmat @ uvec > 0).long()\n",
    "            if uvec_new.sum() == uvec.sum():\n",
    "                break\n",
    "            uvec = uvec_new\n",
    "            dvec += uvec\n",
    "        if uvec.sum() != 0: # there are nodes trapped in loops\n",
    "            raise Warning('When assigning depth, discover the following vertices trapped in loops: {}'.format(torch.nonzero(uvec,as_tuple=True)[0].tolist()))\n",
    "        return dvec\n",
    "    \n",
    "    def add_self_loops(self, start = 0):\n",
    "        # start: the sarting node from which on the self-loop should be added\n",
    "        assert self.dims[0] == self.dims[1], 'add_self_loops can only be called with square adjacency matrix.'\n",
    "        loops = torch.arange(start, self.dims[0])\n",
    "        indices_prepend = torch.stack([loops, loops])\n",
    "        edge_types_prepend = torch.ones(loops.shape, dtype=torch.long)\n",
    "        indices = torch.cat([indices_prepend, self.indices], -1)\n",
    "        edge_types = torch.cat([edge_types_prepend, self.edge_types+1], -1)\n",
    "        return Graph(self.dims, indices, edge_types, self.source_depths)\n",
    "    \n",
    "    def expand(self, target_dim, source_dim):\n",
    "        # prepare views\n",
    "        indices = self.indices.view(2,-1,1,1)\n",
    "        edge_types = self.edge_types.view(-1,1,1)\n",
    "        target_inds = torch.arange(target_dim).view(-1,1)\n",
    "        source_inds = torch.arange(source_dim).view(1,-1)\n",
    "        # calculate indices extension\n",
    "        target_inds_ext = indices[0,...] * target_dim + target_inds\n",
    "        source_inds_ext = indices[1,...] * source_dim + source_inds\n",
    "        # calculate edge type extension\n",
    "        edge_types_ext = ((edge_types - 1) * target_dim + target_inds) * source_dim + source_inds + 1\n",
    "        # expand and flatten tensor\n",
    "        target_inds_ext = target_inds_ext.expand(edge_types_ext.shape).flatten()\n",
    "        source_inds_ext = source_inds_ext.expand(edge_types_ext.shape).flatten()\n",
    "        edge_types_ext = edge_types_ext.flatten()\n",
    "        # expand depths (to the source side)\n",
    "        source_depths_ext = self.source_depths.repeat_interleave(source_dim)\n",
    "        dims_ext = (self.dims[0] * target_dim, self.dims[1] * source_dim)\n",
    "        indices_ext = torch.stack([target_inds_ext, source_inds_ext])\n",
    "        return Graph(dims_ext, indices_ext, edge_types_ext, source_depths_ext)\n",
    "    \n",
    "    def sparse_matrix(self, vector, depth = None):\n",
    "        if depth is None:\n",
    "            indices = self.indices\n",
    "            edge_types = self.edge_types\n",
    "        else:\n",
    "            select = self.edge_depths == depth\n",
    "            indices = self.indices[:, select]\n",
    "            edge_types = self.edge_types[select]\n",
    "        return torch.sparse_coo_tensor(indices, vector[edge_types-1], self.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 2, 5, 1, 6, 0, 0, 4, 7, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 5, 2, 6, 1, 0, 0, 7, 4, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 2, 5, 0, 0, 1, 6, 0, 0, 4, 7, 0, 0, 0, 0],\n",
       "        [0, 3, 5, 2, 0, 0, 6, 1, 0, 0, 7, 4, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Lattice(4, 2).causal_graph()\n",
    "graph.adjacency_matrix().to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth assignment: a vector $d$, s.t. $d_i$ is the depth of the $i$th node. The depth assingment for both the target and the source sides are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 2, 2, 3, 2, 3, 3, 4, 3, 4, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.source_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  8,  9,  9,  9, 10,\n",
       "         10, 10, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "         13, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15],\n",
       "        [ 1,  2,  1,  2,  1,  1,  3,  2,  1,  1,  3,  2,  4,  1,  5,  1,  3,  2,\n",
       "          1,  6,  7,  1,  3,  2,  1,  9,  3,  5,  4,  8,  2,  1,  9,  3,  5,  4,\n",
       "          8,  2,  7,  1, 11,  6,  3, 10,  7,  2,  1, 11,  6,  3, 10]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 2, 1, 2, 0, 3, 0, 2, 1, 0, 2, 3, 0, 2, 1,\n",
       "        0, 4, 2, 3, 2, 3, 1, 0, 4, 2, 3, 2, 3, 1, 3, 0, 4, 2, 2, 3, 3, 1, 0, 4,\n",
       "        2, 2, 3])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edge_depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add self loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 4, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 4, 0, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 4, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 4, 0, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 4, 3, 6, 2, 7, 0, 0, 5, 8, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 4, 6, 3, 7, 2, 0, 0, 8, 5, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 4, 3, 6, 0, 0, 2, 7, 0, 0, 5, 8, 0, 0, 1, 0],\n",
       "        [0, 4, 6, 3, 0, 0, 7, 2, 0, 0, 8, 5, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_sl = graph.add_self_loops()\n",
    "graph_sl.adjacency_matrix().to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 2]),\n",
       " tensor([[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 1, 0, 0],\n",
       "         [0, 1, 2, 0]]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Lattice(2, 2).causal_graph()\n",
    "graph.source_depths, graph.adjacency_matrix().to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2]),\n",
       " tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  2,  3,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  4,  5,  6,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  2,  3,  7,  8,  9,  0,  0,  0],\n",
       "         [ 0,  0,  0,  4,  5,  6, 10, 11, 12,  0,  0,  0]]))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_ext = graph.expand(2, 3)\n",
    "graph_ext.source_depths, graph_ext.adjacency_matrix().to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2]),\n",
       " tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  2,  3,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  4,  5,  6,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  7,  8,  9,  1,  2,  3,  0,  0,  0],\n",
       "         [ 0,  0,  0, 10, 11, 12,  4,  5,  6,  0,  0,  0],\n",
       "         [ 0,  0,  0,  7,  8,  9, 13, 14, 15,  1,  2,  3],\n",
       "         [ 0,  0,  0, 10, 11, 12, 16, 17, 18,  4,  5,  6]]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_ext = graph.add_self_loops().expand(2, 3)\n",
    "graph_ext.source_depths, graph_ext.adjacency_matrix().to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create depth-specific weight matrix from weight vector. Data moves to GPU automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  -6.,  -4.,   3.,  -4.,  -2.,  -5.,   3.,   0.,   2., -11.,  15.,\n",
       "          3.,   2.,  -7.,   0.,  -1., -17.], device='cuda:0')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_vector = (torch.randn(graph_ext.max_edge_type)*10).round().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., -12.,  -6.,  -4.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   3.,  -4.,  -2.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,  -5.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   2., -11.,  15.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,  -5.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   2., -11.,  15.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_ext.sparse_matrix(weight_vector, 0).to_dense().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    \"\"\" Graph Convolution layer \n",
    "        \n",
    "        Args:\n",
    "        graph: graph object\n",
    "        in_features: number of input features (per node)\n",
    "        out_features: number of output features (per node)\n",
    "        bias: whether to learn an edge-depenent bias\n",
    "        self_loop: whether to include self loops in message passing\n",
    "    \"\"\"\n",
    "    def __init__(self, graph:Graph, in_features:int, out_features:int,\n",
    "                 bias:bool = True, self_loop:bool or int = True):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.self_loop = self_loop\n",
    "        if isinstance(self.self_loop, bool):\n",
    "            if self.self_loop:\n",
    "                self.graph = graph.add_self_loops()\n",
    "            else:\n",
    "                self.graph = graph\n",
    "        else:\n",
    "            self.graph = graph.add_self_loops(start=self.self_loop)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_graph = self.graph.expand(self.out_features, self.in_features)\n",
    "        self.weight_vector = nn.Parameter(torch.Tensor(self.weight_graph.max_edge_type))\n",
    "        self.bias = bias\n",
    "        if self.bias:\n",
    "            self.bias_graph = self.graph.expand(out_features, 1)\n",
    "            self.bias_vector = nn.Parameter(torch.Tensor(self.bias_graph.max_edge_type))        \n",
    "        self.reset_parameters()\n",
    "        (self.target_dim, self.source_dim) = self.weight_graph.dims\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / math.sqrt(self.in_features)\n",
    "        nn.init.uniform_(self.weight_vector, -bound, bound)\n",
    "        if self.bias:\n",
    "            nn.init.uniform_(self.bias_vector, -bound, bound)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}, self_loop={}\\n{}'.format(\n",
    "            self.in_features, self.out_features, self.bias, self.self_loop, self.graph)\n",
    "    \n",
    "    def forward(self, x, depth=None):\n",
    "        weight_matrix = self.weight_graph.sparse_matrix(self.weight_vector, depth)\n",
    "        bias_matrix = self.bias_graph.sparse_matrix(self.bias_vector, depth)\n",
    "        unit = torch.ones((bias_matrix.shape[1], 1), dtype=bias_matrix.dtype, device=bias_matrix.device)\n",
    "        return weight_matrix @ x + bias_matrix @ unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphConvLayer(\n",
       "  in_features=3, out_features=2, bias=True, self_loop=True\n",
       "  Graph(4x4, 7 edges of 3 types)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcl = GraphConvLayer(Lattice(2, 2).causal_graph(), 3, 2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some input. shape: [num_vertices * in_features, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2756,  1.2161,  0.0269, -1.0970,  0.4550],\n",
       "        [ 1.2512,  0.1701, -0.0775,  0.6105,  0.3584],\n",
       "        [ 0.2962, -1.4163, -0.3514, -1.3215,  1.0920],\n",
       "        [-2.3260, -1.8017,  1.3213,  0.9404,  0.8980],\n",
       "        [ 0.0933,  1.0067, -0.5500, -0.0405, -0.3438],\n",
       "        [-1.2374,  1.1529,  1.2877, -0.7187,  0.4455],\n",
       "        [ 0.6294,  0.4120,  0.8329,  1.0898, -0.7481],\n",
       "        [-1.0627, -1.3954,  0.4898,  1.3784, -0.1808],\n",
       "        [ 0.1091, -0.1274,  0.8931,  1.6168,  0.3910],\n",
       "        [-1.6585,  0.1301, -0.8432, -2.2592,  1.1184],\n",
       "        [ 1.1861,  0.6768,  0.1555,  1.3814, -1.3598],\n",
       "        [-0.2948,  2.9379, -0.8041, -0.4929,  0.9091]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(gcl.source_dim,5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5503, -0.4548, -0.5375, -1.4977, -0.3024],\n",
       "        [-0.6669,  0.3509, -0.1501, -0.5594, -0.0157],\n",
       "        [-1.6627, -1.4377,  0.5147, -0.3018,  0.0803],\n",
       "        [-1.0524, -0.9006,  0.3106,  0.2208,  0.1689],\n",
       "        [-0.3420,  0.3581, -0.8189, -0.9046, -1.1280],\n",
       "        [ 1.9121,  0.2078, -0.1099,  0.6334, -0.3435],\n",
       "        [-2.2384,  0.0512, -1.5212, -2.6627,  0.4128],\n",
       "        [ 1.4476,  0.6067, -0.8818, -1.0748,  1.0055]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcl(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward by depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2783,  0.3193, -0.2737, -0.4814, -0.6108],\n",
       "        [ 0.0641, -0.0190,  0.1403,  0.2319, -0.4773],\n",
       "        [ 0.1614,  0.2088,  0.0901, -0.1014,  0.1561],\n",
       "        [ 0.4022,  0.5706, -0.1531, -0.4443,  0.6398]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcl(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvNet(nn.Module):\n",
    "    \"\"\" Graph Convolution network \n",
    "        \n",
    "        Args:\n",
    "        graph: graph object\n",
    "        features: a list of numbers of features (per node) across layers\n",
    "        bias: whether to learn an edge-depenent bias\n",
    "        nonlinearity: nonlinear activation to use\n",
    "    \"\"\"\n",
    "    def __init__(self, graph:Graph, features, bias:bool = True, nonlinearity:str = 'Tanh'):\n",
    "        super(GraphConvNet, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.features = features\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in range(1, len(self.features)):\n",
    "            if l == 1: # the first layer should not have self loops\n",
    "                self.layers.append(GraphConvLayer(self.graph, self.features[0], self.features[1], bias, self_loop=False))\n",
    "            else: # remaining layers are normal\n",
    "                self.layers.append(getattr(nn, nonlinearity)()) # activatioin layer\n",
    "                self.layers.append(GraphConvLayer(self.graph, self.features[l - 1], self.features[l], bias, self_loop=1))\n",
    "                \n",
    "    def forward(self, input, depth=None, cache=None):\n",
    "        # input: [..., nodes, features]\n",
    "        in_shape = input.shape\n",
    "        batch_dim = torch.tensor(in_shape[:-2]).prod()\n",
    "        input_dim = torch.tensor(in_shape[-2:]).prod()\n",
    "        x = input.view((batch_dim, input_dim)).T\n",
    "        if depth is None:\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "        else: # depth-specific forward\n",
    "            if cache is None: # if cache not exist, prepare cache\n",
    "                cache = [x]\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, GraphConvLayer):\n",
    "                        target_dim = layer.target_dim\n",
    "                    cache.append(torch.zeros((target_dim, batch_dim), device=x.device))\n",
    "            else: # if cache exist, load x to cache[0]\n",
    "                cache[0] = x\n",
    "            # cache is ready, start forwarding\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                if isinstance(layer, GraphConvLayer):\n",
    "                    if l == 0: # first layer should forward from the previous depth\n",
    "                        cache[l+1] = cache[l+1] + layer(cache[l], depth - 1)\n",
    "                    else: # remaining layer forward from the current depth\n",
    "                        cache[l+1] = cache[l+1] + layer(cache[l], depth)\n",
    "                else:\n",
    "                    cache[l+1] = layer(cache[l])\n",
    "            x = cache[-1] # last cache hosts output\n",
    "        out_shape = in_shape[:-1]+(self.features[-1],)\n",
    "        output = x.T.view(out_shape)\n",
    "        if cache is None:\n",
    "            return output\n",
    "        else:\n",
    "            return output, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphConvNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): GraphConvLayer(\n",
       "      in_features=2, out_features=4, bias=True, self_loop=False\n",
       "      Graph(4x4, 3 edges of 2 types)\n",
       "    )\n",
       "    (1): Tanh()\n",
       "    (2): GraphConvLayer(\n",
       "      in_features=4, out_features=3, bias=True, self_loop=1\n",
       "      Graph(4x4, 6 edges of 3 types)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn = GraphConvNet(Lattice(2, 2).causal_graph(), (2, 4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foward by depth iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- depth: 0 ----\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1460, -0.1877, -0.0758],\n",
      "         [-0.0525,  0.2649, -0.2509],\n",
      "         [-0.0525,  0.2649, -0.2509]]], grad_fn=<ViewBackward>)\n",
      "---- depth: 1 ----\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1460, -0.1877, -0.0758],\n",
      "         [ 0.2149, -0.2660, -0.6753],\n",
      "         [ 0.9043,  0.2574, -0.0924]]], grad_fn=<ViewBackward>)\n",
      "---- depth: 2 ----\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1460, -0.1877, -0.0758],\n",
      "         [ 0.2149, -0.2660, -0.6753],\n",
      "         [ 1.3022, -0.0668, -0.7082]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,4,2)\n",
    "cache = None\n",
    "for depth in range(gcn.graph.max_depth+1):\n",
    "    y, cache = gcn(x, depth, cache)\n",
    "    print('---- depth: {} ----'.format(depth))\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth-wise forward and one-shot forward result in the same output (upto roundoff error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 1.4901e-08, -1.4901e-08,  0.0000e+00],\n",
       "         [ 0.0000e+00,  4.4703e-08, -5.9605e-08]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  1.4901e-08, -5.9605e-08]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  2.9802e-08,  0.0000e+00],\n",
       "         [ 0.0000e+00,  5.9605e-08, -5.9605e-08]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3,4,2)\n",
    "cache = None\n",
    "for depth in range(gcn.graph.max_depth+1):\n",
    "    y, cache = gcn(x, depth, cache)\n",
    "gcn(x) - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoregressive(nn.Module, dist.Distribution):\n",
    "    \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n",
    "        \n",
    "        Args:\n",
    "        latt: Lattice\n",
    "        order: group order\n",
    "        hidden_features: a list of integers specifying hidden dimensions\n",
    "        radius (optional): radius used to construct causal graph \n",
    "        bias, nonlinearity (optional): to set graph convolutional network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latt:Lattice, order:int, hidden_features, radius=1., bias:bool = True, nonlinearity:str = 'Tanh'):\n",
    "        super(Autoregressive, self).__init__()\n",
    "        self.latt = latt\n",
    "        self.order = order\n",
    "        self.graph = latt.causal_graph(radius)\n",
    "        features = [order] + hidden_features + [order]\n",
    "        self.gcn = GraphConvNet(self.graph, features, bias, nonlinearity)\n",
    "        self.sampler = dist.OneHotCategorical\n",
    "    \n",
    "    def log_prob(self, sample):\n",
    "        logits = self.gcn(sample) # forward pass to get logits\n",
    "        return torch.sum(sample * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "    \n",
    "    def sample(self, sample_size: int):\n",
    "        device = next(self.parameters()).device # determine device\n",
    "        samples = torch.zeros(sample_size, self.latt.sites, self.order, device=device) # prepare sample container\n",
    "        cache = None\n",
    "        for depth in range(self.graph.max_depth + 1):\n",
    "            logits, cache = self.gcn(samples, depth, cache)\n",
    "            select = self.graph.source_depths == depth # select nodes of the depth\n",
    "            samples[...,select,:] = self.sampler(logits=logits[...,select,:]).sample() # sample from logits\n",
    "            print('--- depth: {} ---'.format(depth))\n",
    "            print(logits.data)\n",
    "            print(samples.data)\n",
    "        print('---')\n",
    "        print(self.gcn(samples).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- depth: 0 ---\n",
      "tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.4735, -0.3400],\n",
      "         [-0.2883,  0.1043],\n",
      "         [-0.2883,  0.1043]]])\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "--- depth: 1 ---\n",
      "tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.4735, -0.3400],\n",
      "         [ 0.1348, -0.1086],\n",
      "         [ 0.3147,  0.8451]]])\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]]])\n",
      "--- depth: 2 ---\n",
      "tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.4735, -0.3400],\n",
      "         [ 0.1348, -0.1086],\n",
      "         [ 0.9257,  0.5878]]])\n",
      "tensor([[[0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]]])\n",
      "---\n",
      "tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.4735, -0.3400],\n",
      "         [ 0.1348, -0.1086],\n",
      "         [ 0.9257,  0.5878]]])\n"
     ]
    }
   ],
   "source": [
    "ag = Autoregressive(Lattice(2, 2), 2, [4])\n",
    "ag.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoregressive(nn.Module, dist.Distribution):\n",
    "    \"\"\" Represent a generative model that can generate samples and evaluate log probabilities.\n",
    "        \n",
    "        Args:\n",
    "        latt: Lattice\n",
    "        order: group order\n",
    "        hidden_features: a list of integers specifying hidden dimensions\n",
    "        radius (optional): radius used to construct causal graph \n",
    "        bias, nonlinearity (optional): to set graph convolutional network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latt:Lattice, order:int, hidden_features, radius=1., bias:bool = True, nonlinearity:str = 'Tanh'):\n",
    "        super(Autoregressive, self).__init__()\n",
    "        self.latt = latt\n",
    "        self.order = order\n",
    "        self.graph = latt.causal_graph(radius)\n",
    "        features = [order] + hidden_features + [order]\n",
    "        self.gcn = GraphConvNet(self.graph, features, bias, nonlinearity)\n",
    "        self.sampler = dist.OneHotCategorical\n",
    "    \n",
    "    def log_prob(self, samples):\n",
    "        logits = self.gcn(samples) # forward pass to get logits\n",
    "        log_prob = torch.sum(samples * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "        return log_prob\n",
    "    \n",
    "    def sample(self, sample_size: int, return_log_prob:bool = False):\n",
    "        device = next(self.parameters()).device # determine device\n",
    "        samples = torch.zeros(sample_size, self.latt.sites, self.order, device=device) # prepare sample container\n",
    "        cache = None\n",
    "        for depth in range(self.graph.max_depth + 1):\n",
    "            logits, cache = self.gcn(samples, depth, cache)\n",
    "            select = self.graph.source_depths == depth # select nodes of the depth\n",
    "            samples[...,select,:] = self.sampler(logits=logits[...,select,:]).sample() # sample from logits\n",
    "        if return_log_prob:\n",
    "            log_prob = torch.sum(samples * F.log_softmax(logits, dim=-1), (-2,-1))\n",
    "            return samples, log_prob\n",
    "        else:\n",
    "            return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 1.],\n",
       "          [0., 1.],\n",
       "          [0., 1.],\n",
       "          [1., 0.]],\n",
       " \n",
       "         [[1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.]]]),\n",
       " tensor([-3.1582, -1.7350], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag = Autoregressive(Lattice(2, 2), 2, [4])\n",
    "ag.sample(2, return_log_prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node object represents a single node in the lattice. Properties:\n",
    "- type: `'lat'` - latent node, `'phy'` - physical node.\n",
    "- ind: node index.\n",
    "- center: $(x,y)$ coordinate projected to the boundary coordinate system.\n",
    "- generation: generation of node in the hyperbolic structure.\n",
    "- parent: point to the parent node (except for node 0).\n",
    "- children: (*latent only*) [ch1, ch2] a pair of children nodes.\n",
    "- site: (*physical only*) site index of the physical node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\" Represent a node object, containing coordinate and relationship information.\n",
    "    \"\"\"\n",
    "    def __init__(self, ind:int):\n",
    "        self.type = None\n",
    "        self.ind = ind\n",
    "        self.center = None\n",
    "        self.generation = None\n",
    "        self.parent = None\n",
    "        self.children = [None, None]\n",
    "        self.site = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Node({})'.format(self.ind)\n",
    "    \n",
    "    def ancestors(self):\n",
    "        # ancestor = self + ancestor of parent\n",
    "        if self.parent is not None:\n",
    "            return [self] + self.parent.ancestors()\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def shadow_sites(self):\n",
    "        # shadow_sites = sum of shadow_sites of children\n",
    "        if self.type is 'lat':\n",
    "            shd = []\n",
    "            for node in self.children:\n",
    "                shd += node.shadow_sites()\n",
    "            return shd\n",
    "        elif self.type is 'phy':\n",
    "            return [self.site]\n",
    "        \n",
    "    def action_sites(self):\n",
    "        # action_sites = shadow_sites of last child\n",
    "        if self.type is 'lat':\n",
    "            return self.children[-1].shadow_sites()\n",
    "        elif self.type is 'phy':\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(0), Node(1), Node(2), Node(3)]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Node(i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lattice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lattice object host a list of nodes. Nodes are classified by types: latent and physical. The physical node is associated with site index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lattice(object):\n",
    "    \"\"\" Hosts lattice information and construct causal graph\n",
    "        \n",
    "        Args:\n",
    "        size: number of size along one dimension (assuming square/cubical lattice)\n",
    "              must be a power of 2 for binary tree construction\n",
    "        dimension: dimension of the lattice\n",
    "    \"\"\"\n",
    "    def __init__(self, size:int, dimension:int):\n",
    "        assert (size & (size-1) == 0) and size != 0, \"size must be a power of 2.\"\n",
    "        assert dimension > 0, \"dimension must be a positive integer.\"\n",
    "        self.size = size\n",
    "        self.dimension = dimension\n",
    "        self.sites = size**dimension\n",
    "        self.nodes = [Node(i) for i in range(2*self.sites)]\n",
    "        self.nodes[0].type = 'lat'\n",
    "        self.nodes[0].generation = 0\n",
    "        self.nodes[0].children = [self.nodes[1]]\n",
    "        self.nodes[1].parent = self.nodes[0]\n",
    "        def partition(rng: torch.Tensor, dim: int, ind: int, gen: int):\n",
    "            this_node = self.nodes[ind]\n",
    "            this_node.center = rng.float().mean(-1)\n",
    "            this_node.generation = gen\n",
    "            if rng[dim].sum()%2 == 0:\n",
    "                this_node.type = 'lat'\n",
    "                mid = rng[dim].sum()//2\n",
    "                rng1 = rng.clone()\n",
    "                rng1[dim, 1] = mid\n",
    "                rng2 = rng.clone()\n",
    "                rng2[dim, 0] = mid\n",
    "                ind1 = (ind-2**gen)+2*2**gen\n",
    "                ind2 = (ind-2**gen)+3*2**gen\n",
    "                partition(rng1, (dim + 1)%self.dimension, ind1, gen+1)\n",
    "                partition(rng2, (dim + 1)%self.dimension, ind2, gen+1)\n",
    "                this_node.children = [self.nodes[ind1], self.nodes[ind2]]\n",
    "                self.nodes[ind1].parent = this_node\n",
    "                self.nodes[ind2].parent = this_node\n",
    "            else:\n",
    "                this_node.type = 'phy'\n",
    "                this_node.site = rng[:,0].dot(self.size**torch.arange(0,self.dimension).flip(0)).item()\n",
    "        partition(torch.tensor([[0, self.size]]*self.dimension), 0, 1, 0)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Lattice({} grid)'.format('x'.join(str(self.size) for k in range(self.dimension)))\n",
    "    \n",
    "    def wavelet_maps(self):\n",
    "        decoder_map = torch.zeros((self.sites,self.sites), dtype=torch.long)\n",
    "        for node in self.nodes:\n",
    "            if node.type is 'lat':\n",
    "                source = node.ind\n",
    "                for target in node.action_sites():\n",
    "                    decoder_map[target, source] = 1\n",
    "        encoder_map = torch.inverse(decoder_map.double()).round().long()\n",
    "        return encoder_map, decoder_map\n",
    "                    \n",
    "    def relevant_nodes(self, node, radius = 1.):\n",
    "        # relevant_nodes = union of ancestors of adjacent nodes within given radius\n",
    "        scaled_radius = radius * self.size / 2**(node.generation/self.dimension)\n",
    "        relevant_nodes = set()\n",
    "        for prior_node in self.nodes[1:node.ind]:\n",
    "            displacement = prior_node.center - node.center\n",
    "            displacement = (displacement + self.size/2)%self.size - self.size/2\n",
    "            if displacement.norm() < scaled_radius:\n",
    "                relevant_nodes.update(prior_node.ancestors())\n",
    "        return relevant_nodes\n",
    "    \n",
    "    def common_ancestor(self, node1, node2):\n",
    "        # the closest common ancestor of two nodes\n",
    "        common_ancestor = None\n",
    "        while common_ancestor is None:\n",
    "            if node1.generation == node2.generation:\n",
    "                if node1 is node2:\n",
    "                    common_ancestor = node1\n",
    "                else:\n",
    "                    node1 = node1.parent\n",
    "                    node2 = node2.parent\n",
    "            elif node1.generation < node2.generation:\n",
    "                node2 = node2.parent\n",
    "            else: # node1.generation > node2.generation\n",
    "                node1 = node1.parent\n",
    "        return common_ancestor\n",
    "    \n",
    "    def relationship(self, node1, node2):\n",
    "        common_ancestor = self.common_ancestor(node1, node2)\n",
    "        return (node1.generation - common_ancestor.generation, node2.generation - common_ancestor.generation)\n",
    "    \n",
    "    def causal_graph(self, radius = 1.):\n",
    "        relations = set()\n",
    "        edges = {}\n",
    "        for target_node in self.nodes[1:]:\n",
    "            if target_node.type is 'lat':\n",
    "                for source_node in self.relevant_nodes(target_node, radius):\n",
    "                    relation = self.relationship(source_node, target_node)\n",
    "                    relations.add(relation)\n",
    "                    edges[(target_node.ind, source_node.ind)] = relation\n",
    "        relations = list(relations)\n",
    "        relations.sort()\n",
    "        type_map = {relation: k+1 for k, relation in enumerate(relations)}\n",
    "        indices = torch.zeros((2, len(edges)), dtype = torch.long)\n",
    "        edge_types = torch.zeros(len(edges), dtype = torch.long)\n",
    "        for k, (edge, relation) in enumerate(edges.items()):\n",
    "            indices[0, k] = edge[0]\n",
    "            indices[1, k] = edge[1]\n",
    "            edge_types[k] = type_map[relation]\n",
    "        graph = Graph(self.sites, indices, edge_types)\n",
    "        graph.type_dict = {edge_type: relation for relation, edge_type in type_map.items()}\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind type gen  children parent site\n",
      "  0 lat    0         1            \n",
      "  1 lat    0       2,3      0     \n",
      "  2 lat    1       4,6      1     \n",
      "  3 lat    1       5,7      1     \n",
      "  4 lat    2      8,12      2     \n",
      "  5 lat    2      9,13      3     \n",
      "  6 lat    2     10,14      2     \n",
      "  7 lat    2     11,15      3     \n",
      "  8 lat    3     16,24      4     \n",
      "  9 lat    3     17,25      5     \n",
      " 10 lat    3     18,26      6     \n",
      " 11 lat    3     19,27      7     \n",
      " 12 lat    3     20,28      4     \n",
      " 13 lat    3     21,29      5     \n",
      " 14 lat    3     22,30      6     \n",
      " 15 lat    3     23,31      7     \n",
      " 16 phy    4                8    0\n",
      " 17 phy    4                9    8\n",
      " 18 phy    4               10    2\n",
      " 19 phy    4               11   10\n",
      " 20 phy    4               12    4\n",
      " 21 phy    4               13   12\n",
      " 22 phy    4               14    6\n",
      " 23 phy    4               15   14\n",
      " 24 phy    4                8    1\n",
      " 25 phy    4                9    9\n",
      " 26 phy    4               10    3\n",
      " 27 phy    4               11   11\n",
      " 28 phy    4               12    5\n",
      " 29 phy    4               13   13\n",
      " 30 phy    4               14    7\n",
      " 31 phy    4               15   15\n"
     ]
    }
   ],
   "source": [
    "latt = Lattice(4,2)\n",
    "print('ind type gen  children parent site')\n",
    "for node in latt.nodes:\n",
    "    print('{:3d} {:>3s} {:4d} {:>9s} {:>6s} {:>4s}'.format(node.ind, node.type, node.generation, ','.join(str(c.ind) for c in node.children if c is not None), str(node.parent.ind) if node.parent is not None else '', str(node.site) if node.site is not None else ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites on which each latent variable acts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind    action_sites\n",
      "  0 -> [0, 1, 4, 5, 2, 3, 6, 7, 8, 9, 12, 13, 10, 11, 14, 15]\n",
      "  1 -> [8, 9, 12, 13, 10, 11, 14, 15]\n",
      "  2 -> [2, 3, 6, 7]\n",
      "  3 -> [10, 11, 14, 15]\n",
      "  4 -> [4, 5]\n",
      "  5 -> [12, 13]\n",
      "  6 -> [6, 7]\n",
      "  7 -> [14, 15]\n",
      "  8 -> [1]\n",
      "  9 -> [9]\n",
      " 10 -> [3]\n",
      " 11 -> [11]\n",
      " 12 -> [5]\n",
      " 13 -> [13]\n",
      " 14 -> [7]\n",
      " 15 -> [15]\n"
     ]
    }
   ],
   "source": [
    "print('ind    action_sites')\n",
    "for node in latt.nodes:\n",
    "    if node.type is 'lat':\n",
    "        print('{:3d} -> {}'.format(node.ind, node.action_sites()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder map $e$ and decoder map $d$ of wavelet transform, s.t.\n",
    "$$\\text{encode: }z_a=\\prod_i x_i^{e_{a i}},\\quad\\text{decode: } x_i=\\prod_a z_a^{d_{i a}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [-1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [-1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  1,  0,  0,  0,  0,  0],\n",
       "         [-1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  1,  0,  0,  0],\n",
       "         [ 0,  0, -1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  1,  0],\n",
       "         [-1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  1,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  1]]),\n",
       " tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt.wavelet_maps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Relevant nodes** of a given node is defined to be the nodes in the *past light-cone* of the *vicinity* of the given node within a given relative radius. \n",
    "- Given a target node, the **causal relation** will be established from its relavant nodes to itself.\n",
    "- The **edge type** can be determined by the relationship between the nodes. The **relationship** between a pair of node is given by their *relative generation* to their *closest common ancestor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind relevant_nodes\n",
      "  1 set()\n",
      "  2 {Node(1)}\n",
      "  3 {Node(2), Node(1)}\n",
      "  4 {Node(2), Node(1), Node(3)}\n",
      "  5 {Node(2), Node(1), Node(4), Node(3)}\n",
      "  6 {Node(2), Node(1), Node(5), Node(4), Node(3)}\n",
      "  7 {Node(2), Node(1), Node(5), Node(6), Node(4), Node(3)}\n",
      "  8 {Node(2), Node(1), Node(5), Node(6), Node(4), Node(3)}\n",
      "  9 {Node(2), Node(1), Node(5), Node(8), Node(4), Node(3), Node(7)}\n",
      " 10 {Node(2), Node(1), Node(8), Node(6), Node(4), Node(3), Node(7)}\n",
      " 11 {Node(2), Node(1), Node(5), Node(10), Node(6), Node(9), Node(3), Node(7)}\n",
      " 12 {Node(2), Node(1), Node(5), Node(8), Node(6), Node(4), Node(9), Node(3)}\n",
      " 13 {Node(2), Node(1), Node(5), Node(8), Node(12), Node(4), Node(9), Node(3), Node(7)}\n",
      " 14 {Node(2), Node(1), Node(10), Node(11), Node(12), Node(6), Node(4), Node(3), Node(7)}\n",
      " 15 {Node(2), Node(1), Node(5), Node(10), Node(11), Node(14), Node(6), Node(3), Node(7), Node(13)}\n"
     ]
    }
   ],
   "source": [
    "print('ind relevant_nodes')\n",
    "for node in latt.nodes[1:]:\n",
    "    if node.type is 'lat':\n",
    "        print('{:3d} {}'.format(node.ind, latt.relevant_nodes(node, 1.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: (0, 1), 2: (0, 2), 3: (0, 3), 4: (1, 1), 5: (1, 3), 6: (2, 3), 7: (3, 3)},\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 2, 5, 1, 6, 0, 0, 4, 7, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 5, 2, 6, 1, 0, 0, 7, 4, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 3, 2, 5, 0, 0, 1, 6, 0, 0, 4, 7, 0, 0, 0, 0],\n",
       "         [0, 3, 5, 2, 0, 0, 6, 1, 0, 0, 7, 4, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = latt.causal_graph()\n",
    "graph.type_dict, graph.adjacency_matrix().to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the radius will include more edges to the causal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Graph(16x16, 51 edges of 7 types),\n",
       " Graph(16x16, 85 edges of 9 types),\n",
       " Graph(16x16, 101 edges of 9 types))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latt.causal_graph(1.), latt.causal_graph(1.5), latt.causal_graph(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
